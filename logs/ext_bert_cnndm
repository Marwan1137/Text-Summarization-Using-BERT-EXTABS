[2024-04-15 16:38:19,258 INFO] Device ID 0
[2024-04-15 16:38:19,258 INFO] Device cuda
[2024-04-15 16:38:21,015 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to C:\Users\marwa\AppData\Local\Temp\tmpje62gtt1
[2024-04-15 16:38:21,600 INFO] copying C:\Users\marwa\AppData\Local\Temp\tmpje62gtt1 to cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:38:21,601 INFO] creating metadata file for ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:38:21,602 INFO] removing temp file C:\Users\marwa\AppData\Local\Temp\tmpje62gtt1
[2024-04-15 16:38:21,603 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:38:21,608 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 16:38:22,215 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\Users\marwa\AppData\Local\Temp\tmpduvyri5p
[2024-04-15 16:42:36,744 INFO] copying C:\Users\marwa\AppData\Local\Temp\tmpduvyri5p to cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:42:37,169 INFO] creating metadata file for ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:42:37,170 INFO] removing temp file C:\Users\marwa\AppData\Local\Temp\tmpduvyri5p
[2024-04-15 16:42:37,210 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:42:41,616 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 16:42:41,635 INFO] * number of parameters: 120512513
[2024-04-15 16:42:41,635 INFO] Start training...
[2024-04-15 16:45:40,837 INFO] Device ID 0
[2024-04-15 16:45:40,837 INFO] Device cuda
[2024-04-15 16:45:42,575 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:45:42,576 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 16:45:43,203 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:45:46,134 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 16:45:46,143 INFO] * number of parameters: 120512513
[2024-04-15 16:45:46,144 INFO] Start training...
[2024-04-15 16:53:08,197 INFO] Device ID 0
[2024-04-15 16:53:08,197 INFO] Device cuda
[2024-04-15 16:53:09,867 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:53:09,869 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 16:53:10,462 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:53:13,421 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 16:53:13,433 INFO] * number of parameters: 120512513
[2024-04-15 16:53:13,434 INFO] Start training...
[2024-04-15 16:58:20,935 INFO] Device ID 0
[2024-04-15 16:58:20,936 INFO] Device cuda
[2024-04-15 16:58:23,879 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:58:23,880 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 16:58:24,496 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:58:27,096 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 16:58:27,106 INFO] * number of parameters: 120512513
[2024-04-15 16:58:27,106 INFO] Start training...
[2024-04-15 16:59:05,334 INFO] Device ID 0
[2024-04-15 16:59:05,335 INFO] Device cuda
[2024-04-15 16:59:07,235 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 16:59:07,238 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 16:59:07,866 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 16:59:10,509 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 16:59:10,518 INFO] * number of parameters: 120512513
[2024-04-15 16:59:10,518 INFO] Start training...
[2024-04-15 17:03:11,597 INFO] Device ID 0
[2024-04-15 17:03:11,597 INFO] Device cuda
[2024-04-15 17:03:13,282 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:03:13,284 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:03:13,912 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:03:16,502 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:03:16,510 INFO] * number of parameters: 120512513
[2024-04-15 17:03:16,511 INFO] Start training...
[2024-04-15 17:16:05,035 INFO] Device ID 0
[2024-04-15 17:16:05,035 INFO] Device cuda
[2024-04-15 17:16:06,977 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:16:06,978 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:16:08,104 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:16:10,697 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:16:10,708 INFO] * number of parameters: 120512513
[2024-04-15 17:16:10,708 INFO] Start training...
[2024-04-15 17:19:23,924 INFO] Device ID 0
[2024-04-15 17:19:23,924 INFO] Device cuda
[2024-04-15 17:19:25,647 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:19:25,648 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:19:26,264 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:19:28,944 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:19:28,954 INFO] * number of parameters: 120512513
[2024-04-15 17:19:28,955 INFO] Start training...
[2024-04-15 17:21:09,251 INFO] Device ID 0
[2024-04-15 17:21:09,251 INFO] Device cuda
[2024-04-15 17:21:10,928 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:21:10,929 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:21:11,551 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:21:14,288 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:21:14,297 INFO] * number of parameters: 120512513
[2024-04-15 17:21:14,297 INFO] Start training...
[2024-04-15 17:21:14,432 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-15 17:22:58,433 INFO] Device ID 0
[2024-04-15 17:22:58,433 INFO] Device cuda
[2024-04-15 17:23:00,150 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:23:00,151 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:23:00,793 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:23:03,399 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:23:03,409 INFO] * number of parameters: 120512513
[2024-04-15 17:23:03,409 INFO] Start training...
[2024-04-15 17:23:03,542 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-15 17:23:26,716 INFO] Device ID 0
[2024-04-15 17:23:26,716 INFO] Device cuda
[2024-04-15 17:23:28,474 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:23:28,475 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:23:29,146 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:23:31,829 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:23:31,839 INFO] * number of parameters: 120512513
[2024-04-15 17:23:31,839 INFO] Start training...
[2024-04-15 17:23:31,971 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-15 17:23:48,689 INFO] Device ID 0
[2024-04-15 17:23:48,689 INFO] Device cuda
[2024-04-15 17:23:50,450 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:23:50,452 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:23:51,049 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:23:53,797 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:23:53,806 INFO] * number of parameters: 120512513
[2024-04-15 17:23:53,807 INFO] Start training...
[2024-04-15 17:23:53,944 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-15 17:25:03,494 INFO] Device ID 0
[2024-04-15 17:25:03,495 INFO] Device cuda
[2024-04-15 17:25:05,182 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2024-04-15 17:25:05,184 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-15 17:25:05,828 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2024-04-15 17:25:08,412 INFO] ExtSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (ext_layer): ExtTransformerEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-15 17:25:08,422 INFO] * number of parameters: 120512513
[2024-04-15 17:25:08,423 INFO] Start training...
[2024-04-15 17:25:08,555 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-15 17:27:01,903 INFO] Step 50/50000; xent: 10.11; lr: 0.0000001;   9 docs/s;    113 sec
[2024-04-15 17:28:56,742 INFO] Step 100/50000; xent: 6.80; lr: 0.0000002;   9 docs/s;    228 sec
[2024-04-15 17:30:51,941 INFO] Step 150/50000; xent: 4.38; lr: 0.0000003;   9 docs/s;    343 sec
[2024-04-15 17:32:26,991 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.84.bert.pt, number of examples: 2000
[2024-04-15 17:32:52,618 INFO] Step 200/50000; xent: 3.67; lr: 0.0000004;   9 docs/s;    464 sec
[2024-04-15 17:34:49,422 INFO] Step 250/50000; xent: 3.56; lr: 0.0000005;   9 docs/s;    581 sec
[2024-04-15 17:36:44,178 INFO] Step 300/50000; xent: 3.56; lr: 0.0000006;   9 docs/s;    696 sec
[2024-04-15 17:38:37,487 INFO] Step 350/50000; xent: 3.44; lr: 0.0000007;   9 docs/s;    809 sec
[2024-04-15 17:39:48,485 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.39.bert.pt, number of examples: 2000
[2024-04-15 17:40:31,607 INFO] Step 400/50000; xent: 3.49; lr: 0.0000008;   9 docs/s;    923 sec
[2024-04-15 17:42:26,078 INFO] Step 450/50000; xent: 3.41; lr: 0.0000009;   9 docs/s;   1038 sec
[2024-04-15 17:44:20,611 INFO] Step 500/50000; xent: 3.30; lr: 0.0000010;  10 docs/s;   1152 sec
[2024-04-15 17:46:15,326 INFO] Step 550/50000; xent: 3.28; lr: 0.0000011;   9 docs/s;   1267 sec
[2024-04-15 17:47:02,702 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.6.bert.pt, number of examples: 2001
[2024-04-15 17:48:09,026 INFO] Step 600/50000; xent: 3.38; lr: 0.0000012;   9 docs/s;   1380 sec
[2024-04-15 17:50:03,261 INFO] Step 650/50000; xent: 3.32; lr: 0.0000013;   9 docs/s;   1495 sec
[2024-04-15 17:51:57,841 INFO] Step 700/50000; xent: 3.27; lr: 0.0000014;   9 docs/s;   1609 sec
[2024-04-15 17:53:56,726 INFO] Step 750/50000; xent: 3.15; lr: 0.0000015;   9 docs/s;   1728 sec
[2024-04-15 17:54:27,716 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.81.bert.pt, number of examples: 2000
[2024-04-15 17:55:56,460 INFO] Step 800/50000; xent: 3.12; lr: 0.0000016;   9 docs/s;   1848 sec
[2024-04-15 17:57:55,988 INFO] Step 850/50000; xent: 3.24; lr: 0.0000017;   9 docs/s;   1967 sec
[2024-04-15 17:59:53,617 INFO] Step 900/50000; xent: 3.25; lr: 0.0000018;   9 docs/s;   2085 sec
[2024-04-15 18:01:52,570 INFO] Step 950/50000; xent: 3.23; lr: 0.0000019;   9 docs/s;   2204 sec
[2024-04-15 18:01:57,699 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.56.bert.pt, number of examples: 2001
[2024-04-15 18:03:52,817 INFO] Step 1000/50000; xent: 3.22; lr: 0.0000020;   9 docs/s;   2324 sec
[2024-04-15 18:03:52,829 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_1000.pt
[2024-04-15 18:05:52,705 INFO] Step 1050/50000; xent: 3.27; lr: 0.0000021;   9 docs/s;   2444 sec
[2024-04-15 18:07:50,255 INFO] Step 1100/50000; xent: 3.20; lr: 0.0000022;   9 docs/s;   2562 sec
[2024-04-15 18:09:29,502 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.46.bert.pt, number of examples: 2001
[2024-04-15 18:09:47,800 INFO] Step 1150/50000; xent: 3.20; lr: 0.0000023;   9 docs/s;   2679 sec
[2024-04-15 18:11:43,597 INFO] Step 1200/50000; xent: 3.15; lr: 0.0000024;   9 docs/s;   2795 sec
[2024-04-15 18:13:38,119 INFO] Step 1250/50000; xent: 3.15; lr: 0.0000025;   9 docs/s;   2910 sec
[2024-04-15 18:15:33,565 INFO] Step 1300/50000; xent: 3.32; lr: 0.0000026;   9 docs/s;   3025 sec
[2024-04-15 18:16:49,404 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.42.bert.pt, number of examples: 2000
[2024-04-15 18:17:28,401 INFO] Step 1350/50000; xent: 3.16; lr: 0.0000027;   9 docs/s;   3140 sec
[2024-04-15 18:19:22,714 INFO] Step 1400/50000; xent: 3.19; lr: 0.0000028;   9 docs/s;   3254 sec
[2024-04-15 18:21:17,103 INFO] Step 1450/50000; xent: 3.22; lr: 0.0000029;   9 docs/s;   3369 sec
[2024-04-15 18:23:10,718 INFO] Step 1500/50000; xent: 3.14; lr: 0.0000030;   9 docs/s;   3482 sec
[2024-04-15 18:24:01,293 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.16.bert.pt, number of examples: 2001
[2024-04-15 18:25:05,903 INFO] Step 1550/50000; xent: 3.19; lr: 0.0000031;   9 docs/s;   3597 sec
[2024-04-15 18:26:59,368 INFO] Step 1600/50000; xent: 3.17; lr: 0.0000032;   9 docs/s;   3711 sec
[2024-04-15 18:28:52,933 INFO] Step 1650/50000; xent: 3.16; lr: 0.0000033;   9 docs/s;   3824 sec
[2024-04-15 18:30:46,603 INFO] Step 1700/50000; xent: 3.10; lr: 0.0000034;   9 docs/s;   3938 sec
[2024-04-15 18:31:14,708 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.64.bert.pt, number of examples: 2001
[2024-04-15 18:32:41,914 INFO] Step 1750/50000; xent: 3.21; lr: 0.0000035;   9 docs/s;   4053 sec
[2024-04-15 18:34:35,334 INFO] Step 1800/50000; xent: 3.09; lr: 0.0000036;   9 docs/s;   4167 sec
[2024-04-15 18:36:30,151 INFO] Step 1850/50000; xent: 3.12; lr: 0.0000037;  10 docs/s;   4282 sec
[2024-04-15 18:38:23,463 INFO] Step 1900/50000; xent: 3.22; lr: 0.0000038;   9 docs/s;   4395 sec
[2024-04-15 18:38:30,484 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.21.bert.pt, number of examples: 2001
[2024-04-15 18:40:18,313 INFO] Step 1950/50000; xent: 3.14; lr: 0.0000039;   9 docs/s;   4510 sec
[2024-04-15 18:42:11,649 INFO] Step 2000/50000; xent: 3.20; lr: 0.0000040;   9 docs/s;   4623 sec
[2024-04-15 18:42:11,658 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_2000.pt
[2024-04-15 18:44:09,457 INFO] Step 2050/50000; xent: 3.09; lr: 0.0000041;   9 docs/s;   4741 sec
[2024-04-15 18:45:49,856 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.34.bert.pt, number of examples: 2000
[2024-04-15 18:46:05,837 INFO] Step 2100/50000; xent: 3.15; lr: 0.0000042;   9 docs/s;   4857 sec
[2024-04-15 18:48:01,556 INFO] Step 2150/50000; xent: 3.08; lr: 0.0000043;   9 docs/s;   4973 sec
[2024-04-15 18:49:55,447 INFO] Step 2200/50000; xent: 3.10; lr: 0.0000044;   9 docs/s;   5087 sec
[2024-04-15 18:51:50,562 INFO] Step 2250/50000; xent: 3.08; lr: 0.0000045;   9 docs/s;   5202 sec
[2024-04-15 18:53:03,993 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.111.bert.pt, number of examples: 2000
[2024-04-15 18:53:46,650 INFO] Step 2300/50000; xent: 3.08; lr: 0.0000046;   9 docs/s;   5318 sec
[2024-04-15 18:55:41,483 INFO] Step 2350/50000; xent: 3.14; lr: 0.0000047;   9 docs/s;   5433 sec
[2024-04-15 18:57:35,881 INFO] Step 2400/50000; xent: 3.12; lr: 0.0000048;   9 docs/s;   5547 sec
[2024-04-15 18:59:35,784 INFO] Step 2450/50000; xent: 3.02; lr: 0.0000049;   9 docs/s;   5667 sec
[2024-04-15 19:00:29,048 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.82.bert.pt, number of examples: 2001
[2024-04-15 19:01:30,127 INFO] Step 2500/50000; xent: 3.08; lr: 0.0000050;   9 docs/s;   5782 sec
[2024-04-15 19:03:27,205 INFO] Step 2550/50000; xent: 3.15; lr: 0.0000051;   9 docs/s;   5899 sec
[2024-04-15 19:05:22,051 INFO] Step 2600/50000; xent: 3.11; lr: 0.0000052;   9 docs/s;   6013 sec
[2024-04-15 19:07:15,751 INFO] Step 2650/50000; xent: 3.12; lr: 0.0000053;   9 docs/s;   6127 sec
[2024-04-15 19:07:45,806 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.54.bert.pt, number of examples: 2001
[2024-04-15 19:09:10,309 INFO] Step 2700/50000; xent: 3.03; lr: 0.0000054;   9 docs/s;   6242 sec
[2024-04-15 19:11:05,236 INFO] Step 2750/50000; xent: 3.03; lr: 0.0000055;   9 docs/s;   6357 sec
[2024-04-15 19:12:59,794 INFO] Step 2800/50000; xent: 3.07; lr: 0.0000056;   9 docs/s;   6471 sec
[2024-04-15 19:14:56,291 INFO] Step 2850/50000; xent: 3.15; lr: 0.0000057;   9 docs/s;   6588 sec
[2024-04-15 19:15:05,861 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.120.bert.pt, number of examples: 2001
[2024-04-15 19:16:52,509 INFO] Step 2900/50000; xent: 3.09; lr: 0.0000058;   9 docs/s;   6704 sec
[2024-04-15 19:18:48,516 INFO] Step 2950/50000; xent: 2.97; lr: 0.0000059;   9 docs/s;   6820 sec
[2024-04-15 19:20:45,271 INFO] Step 3000/50000; xent: 3.15; lr: 0.0000060;   9 docs/s;   6937 sec
[2024-04-15 19:20:45,282 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_3000.pt
[2024-04-15 19:22:29,911 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.137.bert.pt, number of examples: 2000
[2024-04-15 19:22:43,655 INFO] Step 3050/50000; xent: 3.01; lr: 0.0000061;   9 docs/s;   7055 sec
[2024-04-15 19:24:39,093 INFO] Step 3100/50000; xent: 3.12; lr: 0.0000062;   9 docs/s;   7171 sec
[2024-04-15 19:26:35,499 INFO] Step 3150/50000; xent: 3.01; lr: 0.0000063;   9 docs/s;   7287 sec
[2024-04-15 19:28:30,675 INFO] Step 3200/50000; xent: 3.07; lr: 0.0000064;   9 docs/s;   7402 sec
[2024-04-15 19:29:48,331 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.13.bert.pt, number of examples: 2001
[2024-04-15 19:30:25,492 INFO] Step 3250/50000; xent: 3.00; lr: 0.0000065;   9 docs/s;   7517 sec
[2024-04-15 19:32:21,747 INFO] Step 3300/50000; xent: 3.01; lr: 0.0000066;   9 docs/s;   7633 sec
[2024-04-15 19:34:18,781 INFO] Step 3350/50000; xent: 3.01; lr: 0.0000067;   9 docs/s;   7750 sec
[2024-04-15 19:36:15,397 INFO] Step 3400/50000; xent: 3.03; lr: 0.0000068;   9 docs/s;   7867 sec
[2024-04-15 19:37:12,609 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.135.bert.pt, number of examples: 1999
[2024-04-15 19:38:15,266 INFO] Step 3450/50000; xent: 3.07; lr: 0.0000069;   9 docs/s;   7987 sec
[2024-04-15 19:40:09,701 INFO] Step 3500/50000; xent: 2.93; lr: 0.0000070;   9 docs/s;   8101 sec
[2024-04-15 19:42:05,170 INFO] Step 3550/50000; xent: 2.95; lr: 0.0000071;   9 docs/s;   8217 sec
[2024-04-15 19:44:01,265 INFO] Step 3600/50000; xent: 3.06; lr: 0.0000072;   9 docs/s;   8333 sec
[2024-04-15 19:44:34,029 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.29.bert.pt, number of examples: 1999
[2024-04-15 19:45:57,175 INFO] Step 3650/50000; xent: 2.98; lr: 0.0000073;   9 docs/s;   8449 sec
[2024-04-15 19:47:52,887 INFO] Step 3700/50000; xent: 2.94; lr: 0.0000074;   9 docs/s;   8564 sec
[2024-04-15 19:49:47,808 INFO] Step 3750/50000; xent: 3.08; lr: 0.0000075;   9 docs/s;   8679 sec
[2024-04-15 19:51:42,569 INFO] Step 3800/50000; xent: 2.93; lr: 0.0000076;   9 docs/s;   8794 sec
[2024-04-15 19:51:52,971 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.58.bert.pt, number of examples: 2000
[2024-04-15 19:53:39,558 INFO] Step 3850/50000; xent: 2.98; lr: 0.0000077;   9 docs/s;   8911 sec
[2024-04-15 19:55:34,689 INFO] Step 3900/50000; xent: 3.06; lr: 0.0000078;   9 docs/s;   9026 sec
[2024-04-15 19:57:29,818 INFO] Step 3950/50000; xent: 2.87; lr: 0.0000079;   9 docs/s;   9141 sec
[2024-04-15 19:59:11,757 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.105.bert.pt, number of examples: 2001
[2024-04-15 19:59:25,946 INFO] Step 4000/50000; xent: 2.97; lr: 0.0000080;   9 docs/s;   9257 sec
[2024-04-15 19:59:25,956 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_4000.pt
[2024-04-15 20:01:25,860 INFO] Step 4050/50000; xent: 2.96; lr: 0.0000081;   9 docs/s;   9377 sec
[2024-04-15 20:03:20,480 INFO] Step 4100/50000; xent: 2.92; lr: 0.0000082;   9 docs/s;   9492 sec
[2024-04-15 20:05:15,246 INFO] Step 4150/50000; xent: 2.85; lr: 0.0000083;   9 docs/s;   9607 sec
[2024-04-15 20:06:36,871 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.73.bert.pt, number of examples: 2000
[2024-04-15 20:07:12,598 INFO] Step 4200/50000; xent: 2.98; lr: 0.0000084;   9 docs/s;   9724 sec
[2024-04-15 20:09:09,930 INFO] Step 4250/50000; xent: 2.91; lr: 0.0000085;   9 docs/s;   9841 sec
[2024-04-15 20:11:05,908 INFO] Step 4300/50000; xent: 2.89; lr: 0.0000086;   9 docs/s;   9957 sec
[2024-04-15 20:13:02,238 INFO] Step 4350/50000; xent: 2.95; lr: 0.0000087;   9 docs/s;  10074 sec
[2024-04-15 20:13:59,913 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.106.bert.pt, number of examples: 1999
[2024-04-15 20:14:57,817 INFO] Step 4400/50000; xent: 2.80; lr: 0.0000088;   9 docs/s;  10189 sec
[2024-04-15 20:16:53,162 INFO] Step 4450/50000; xent: 3.00; lr: 0.0000089;   9 docs/s;  10305 sec
[2024-04-15 20:18:48,791 INFO] Step 4500/50000; xent: 2.89; lr: 0.0000090;   9 docs/s;  10420 sec
[2024-04-15 20:20:43,432 INFO] Step 4550/50000; xent: 2.92; lr: 0.0000091;   9 docs/s;  10535 sec
[2024-04-15 20:21:17,312 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.38.bert.pt, number of examples: 2001
[2024-04-15 20:22:37,398 INFO] Step 4600/50000; xent: 2.94; lr: 0.0000092;   9 docs/s;  10649 sec
[2024-04-15 20:24:32,864 INFO] Step 4650/50000; xent: 2.85; lr: 0.0000093;   9 docs/s;  10764 sec
[2024-04-15 20:26:27,403 INFO] Step 4700/50000; xent: 2.92; lr: 0.0000094;   9 docs/s;  10879 sec
[2024-04-15 20:28:23,707 INFO] Step 4750/50000; xent: 2.85; lr: 0.0000095;   9 docs/s;  10995 sec
[2024-04-15 20:28:38,216 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.97.bert.pt, number of examples: 2001
[2024-04-15 20:30:19,489 INFO] Step 4800/50000; xent: 2.84; lr: 0.0000096;   9 docs/s;  11111 sec
[2024-04-15 20:32:14,693 INFO] Step 4850/50000; xent: 2.84; lr: 0.0000097;  10 docs/s;  11226 sec
[2024-04-15 20:34:11,516 INFO] Step 4900/50000; xent: 2.88; lr: 0.0000098;   9 docs/s;  11343 sec
[2024-04-15 20:35:57,977 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.15.bert.pt, number of examples: 1999
[2024-04-15 20:36:07,208 INFO] Step 4950/50000; xent: 2.92; lr: 0.0000099;   9 docs/s;  11459 sec
[2024-04-15 20:38:02,870 INFO] Step 5000/50000; xent: 2.91; lr: 0.0000100;   9 docs/s;  11574 sec
[2024-04-15 20:38:02,879 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_5000.pt
[2024-04-15 20:40:00,731 INFO] Step 5050/50000; xent: 2.86; lr: 0.0000101;   9 docs/s;  11692 sec
[2024-04-15 20:41:57,142 INFO] Step 5100/50000; xent: 2.88; lr: 0.0000102;   9 docs/s;  11809 sec
[2024-04-15 20:43:19,455 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.94.bert.pt, number of examples: 2001
[2024-04-15 20:43:51,685 INFO] Step 5150/50000; xent: 2.82; lr: 0.0000103;   9 docs/s;  11923 sec
[2024-04-15 20:45:47,930 INFO] Step 5200/50000; xent: 2.91; lr: 0.0000104;   9 docs/s;  12039 sec
[2024-04-15 20:47:45,406 INFO] Step 5250/50000; xent: 2.95; lr: 0.0000105;   9 docs/s;  12157 sec
[2024-04-15 20:49:41,698 INFO] Step 5300/50000; xent: 2.85; lr: 0.0000106;   9 docs/s;  12273 sec
[2024-04-15 20:50:43,872 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.62.bert.pt, number of examples: 2001
[2024-04-15 20:51:37,159 INFO] Step 5350/50000; xent: 2.88; lr: 0.0000107;   9 docs/s;  12389 sec
[2024-04-15 20:53:31,540 INFO] Step 5400/50000; xent: 2.85; lr: 0.0000108;   9 docs/s;  12503 sec
[2024-04-15 20:55:29,899 INFO] Step 5450/50000; xent: 2.90; lr: 0.0000109;   9 docs/s;  12621 sec
[2024-04-15 20:57:25,596 INFO] Step 5500/50000; xent: 2.88; lr: 0.0000110;   9 docs/s;  12737 sec
[2024-04-15 20:58:04,232 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.139.bert.pt, number of examples: 2001
[2024-04-15 20:59:21,544 INFO] Step 5550/50000; xent: 2.87; lr: 0.0000111;   9 docs/s;  12853 sec
[2024-04-15 21:01:18,425 INFO] Step 5600/50000; xent: 2.78; lr: 0.0000112;   9 docs/s;  12970 sec
[2024-04-15 21:03:16,126 INFO] Step 5650/50000; xent: 2.80; lr: 0.0000113;   9 docs/s;  13088 sec
[2024-04-15 21:05:13,061 INFO] Step 5700/50000; xent: 2.92; lr: 0.0000114;   9 docs/s;  13205 sec
[2024-04-15 21:05:28,987 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.88.bert.pt, number of examples: 1999
[2024-04-15 21:07:08,491 INFO] Step 5750/50000; xent: 2.91; lr: 0.0000115;   9 docs/s;  13320 sec
[2024-04-15 21:09:06,496 INFO] Step 5800/50000; xent: 2.91; lr: 0.0000116;   9 docs/s;  13438 sec
[2024-04-15 21:11:02,240 INFO] Step 5850/50000; xent: 2.91; lr: 0.0000117;   9 docs/s;  13554 sec
[2024-04-15 21:12:52,812 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.92.bert.pt, number of examples: 2000
[2024-04-15 21:12:57,487 INFO] Step 5900/50000; xent: 2.85; lr: 0.0000118;   9 docs/s;  13669 sec
[2024-04-15 21:14:54,117 INFO] Step 5950/50000; xent: 2.86; lr: 0.0000119;   9 docs/s;  13786 sec
[2024-04-15 21:16:51,145 INFO] Step 6000/50000; xent: 2.83; lr: 0.0000120;   9 docs/s;  13903 sec
[2024-04-15 21:16:51,154 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_6000.pt
[2024-04-15 21:18:51,393 INFO] Step 6050/50000; xent: 2.78; lr: 0.0000121;   9 docs/s;  14023 sec
[2024-04-15 21:20:21,640 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.69.bert.pt, number of examples: 2000
[2024-04-15 21:20:46,496 INFO] Step 6100/50000; xent: 2.83; lr: 0.0000122;   9 docs/s;  14138 sec
[2024-04-15 21:22:42,818 INFO] Step 6150/50000; xent: 2.90; lr: 0.0000123;   9 docs/s;  14254 sec
[2024-04-15 21:24:38,980 INFO] Step 6200/50000; xent: 2.85; lr: 0.0000124;   9 docs/s;  14370 sec
[2024-04-15 21:26:35,667 INFO] Step 6250/50000; xent: 2.86; lr: 0.0000125;   9 docs/s;  14487 sec
[2024-04-15 21:27:44,746 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.0.bert.pt, number of examples: 2001
[2024-04-15 21:28:31,189 INFO] Step 6300/50000; xent: 2.78; lr: 0.0000126;   9 docs/s;  14603 sec
[2024-04-15 21:30:26,149 INFO] Step 6350/50000; xent: 2.87; lr: 0.0000127;   9 docs/s;  14718 sec
[2024-04-15 21:32:22,625 INFO] Step 6400/50000; xent: 2.85; lr: 0.0000128;   9 docs/s;  14834 sec
[2024-04-15 21:34:20,558 INFO] Step 6450/50000; xent: 2.79; lr: 0.0000129;   9 docs/s;  14952 sec
[2024-04-15 21:35:09,662 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.83.bert.pt, number of examples: 1999
[2024-04-15 21:36:18,284 INFO] Step 6500/50000; xent: 2.84; lr: 0.0000130;   9 docs/s;  15070 sec
[2024-04-15 21:38:14,891 INFO] Step 6550/50000; xent: 2.82; lr: 0.0000131;   9 docs/s;  15186 sec
[2024-04-15 21:40:13,071 INFO] Step 6600/50000; xent: 2.84; lr: 0.0000132;   9 docs/s;  15305 sec
[2024-04-15 21:42:10,785 INFO] Step 6650/50000; xent: 2.88; lr: 0.0000133;   9 docs/s;  15422 sec
[2024-04-15 21:42:36,309 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.138.bert.pt, number of examples: 2000
[2024-04-15 21:44:11,901 INFO] Step 6700/50000; xent: 2.80; lr: 0.0000134;   9 docs/s;  15543 sec
[2024-04-15 21:46:09,455 INFO] Step 6750/50000; xent: 2.77; lr: 0.0000135;   9 docs/s;  15661 sec
[2024-04-15 21:48:07,794 INFO] Step 6800/50000; xent: 2.74; lr: 0.0000136;   9 docs/s;  15779 sec
[2024-04-15 21:50:05,386 INFO] Step 6850/50000; xent: 2.84; lr: 0.0000137;   9 docs/s;  15897 sec
[2024-04-15 21:50:08,130 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.72.bert.pt, number of examples: 2000
[2024-04-15 21:52:03,915 INFO] Step 6900/50000; xent: 2.92; lr: 0.0000138;   9 docs/s;  16015 sec
[2024-04-15 21:54:02,653 INFO] Step 6950/50000; xent: 2.75; lr: 0.0000139;   9 docs/s;  16134 sec
[2024-04-15 21:55:59,297 INFO] Step 7000/50000; xent: 2.76; lr: 0.0000140;   9 docs/s;  16251 sec
[2024-04-15 21:55:59,307 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_7000.pt
[2024-04-15 21:57:38,504 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.4.bert.pt, number of examples: 2001
[2024-04-15 21:58:00,402 INFO] Step 7050/50000; xent: 2.83; lr: 0.0000141;   9 docs/s;  16372 sec
[2024-04-15 21:59:58,474 INFO] Step 7100/50000; xent: 2.80; lr: 0.0000142;   9 docs/s;  16490 sec
[2024-04-15 22:01:57,067 INFO] Step 7150/50000; xent: 2.81; lr: 0.0000143;   9 docs/s;  16609 sec
[2024-04-15 22:03:53,534 INFO] Step 7200/50000; xent: 2.74; lr: 0.0000144;   9 docs/s;  16725 sec
[2024-04-15 22:05:07,995 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.25.bert.pt, number of examples: 2001
[2024-04-15 22:05:50,655 INFO] Step 7250/50000; xent: 2.88; lr: 0.0000145;   9 docs/s;  16842 sec
[2024-04-15 22:07:45,818 INFO] Step 7300/50000; xent: 2.81; lr: 0.0000146;   9 docs/s;  16957 sec
[2024-04-15 22:09:41,716 INFO] Step 7350/50000; xent: 2.81; lr: 0.0000147;   9 docs/s;  17073 sec
[2024-04-15 22:11:37,165 INFO] Step 7400/50000; xent: 2.77; lr: 0.0000148;   9 docs/s;  17189 sec
[2024-04-15 22:12:29,734 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.93.bert.pt, number of examples: 1997
[2024-04-15 22:13:32,641 INFO] Step 7450/50000; xent: 2.78; lr: 0.0000149;   9 docs/s;  17304 sec
[2024-04-15 22:15:27,799 INFO] Step 7500/50000; xent: 2.78; lr: 0.0000150;   9 docs/s;  17419 sec
[2024-04-15 22:17:23,512 INFO] Step 7550/50000; xent: 2.79; lr: 0.0000151;   9 docs/s;  17535 sec
[2024-04-15 22:19:19,332 INFO] Step 7600/50000; xent: 2.81; lr: 0.0000152;   9 docs/s;  17651 sec
[2024-04-15 22:19:48,812 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.37.bert.pt, number of examples: 1999
[2024-04-15 22:21:14,239 INFO] Step 7650/50000; xent: 2.81; lr: 0.0000153;   9 docs/s;  17766 sec
[2024-04-15 22:23:08,532 INFO] Step 7700/50000; xent: 2.78; lr: 0.0000154;   9 docs/s;  17880 sec
[2024-04-15 22:25:07,217 INFO] Step 7750/50000; xent: 2.74; lr: 0.0000155;   9 docs/s;  17999 sec
[2024-04-15 22:27:03,834 INFO] Step 7800/50000; xent: 2.83; lr: 0.0000156;   9 docs/s;  18115 sec
[2024-04-15 22:27:11,325 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.80.bert.pt, number of examples: 1999
[2024-04-15 22:29:01,716 INFO] Step 7850/50000; xent: 2.80; lr: 0.0000157;   9 docs/s;  18233 sec
[2024-04-15 22:30:58,171 INFO] Step 7900/50000; xent: 2.86; lr: 0.0000158;   9 docs/s;  18350 sec
[2024-04-15 22:32:53,803 INFO] Step 7950/50000; xent: 2.80; lr: 0.0000159;   9 docs/s;  18465 sec
[2024-04-15 22:34:30,970 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.96.bert.pt, number of examples: 2001
[2024-04-15 22:34:49,352 INFO] Step 8000/50000; xent: 2.74; lr: 0.0000160;   9 docs/s;  18581 sec
[2024-04-15 22:34:49,363 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_8000.pt
[2024-04-15 22:36:47,918 INFO] Step 8050/50000; xent: 2.75; lr: 0.0000161;   9 docs/s;  18699 sec
[2024-04-15 22:38:45,436 INFO] Step 8100/50000; xent: 2.84; lr: 0.0000162;   9 docs/s;  18817 sec
[2024-04-15 22:40:41,364 INFO] Step 8150/50000; xent: 2.80; lr: 0.0000163;   9 docs/s;  18933 sec
[2024-04-15 22:41:55,750 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.12.bert.pt, number of examples: 2001
[2024-04-15 22:42:37,737 INFO] Step 8200/50000; xent: 2.81; lr: 0.0000164;   9 docs/s;  19049 sec
[2024-04-15 22:44:34,626 INFO] Step 8250/50000; xent: 2.89; lr: 0.0000165;   9 docs/s;  19166 sec
[2024-04-15 22:46:32,247 INFO] Step 8300/50000; xent: 2.82; lr: 0.0000166;   9 docs/s;  19284 sec
[2024-04-15 22:48:28,581 INFO] Step 8350/50000; xent: 2.88; lr: 0.0000167;   9 docs/s;  19400 sec
[2024-04-15 22:49:24,307 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.114.bert.pt, number of examples: 1998
[2024-04-15 22:50:24,887 INFO] Step 8400/50000; xent: 2.87; lr: 0.0000168;   9 docs/s;  19516 sec
[2024-04-15 22:52:20,978 INFO] Step 8450/50000; xent: 2.86; lr: 0.0000169;   9 docs/s;  19632 sec
[2024-04-15 22:54:16,386 INFO] Step 8500/50000; xent: 2.79; lr: 0.0000170;   9 docs/s;  19748 sec
[2024-04-15 22:56:12,856 INFO] Step 8550/50000; xent: 2.86; lr: 0.0000171;   9 docs/s;  19864 sec
[2024-04-15 22:56:43,578 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.95.bert.pt, number of examples: 2001
[2024-04-15 22:58:09,344 INFO] Step 8600/50000; xent: 2.85; lr: 0.0000172;   9 docs/s;  19981 sec
[2024-04-15 23:00:05,156 INFO] Step 8650/50000; xent: 2.87; lr: 0.0000173;   9 docs/s;  20097 sec
[2024-04-15 23:02:04,606 INFO] Step 8700/50000; xent: 2.83; lr: 0.0000174;   9 docs/s;  20216 sec
[2024-04-15 23:04:00,491 INFO] Step 8750/50000; xent: 2.82; lr: 0.0000175;   9 docs/s;  20332 sec
[2024-04-15 23:04:12,597 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.113.bert.pt, number of examples: 1999
[2024-04-15 23:05:58,231 INFO] Step 8800/50000; xent: 2.84; lr: 0.0000176;   9 docs/s;  20450 sec
[2024-04-15 23:07:54,835 INFO] Step 8850/50000; xent: 2.89; lr: 0.0000177;   9 docs/s;  20566 sec
[2024-04-15 23:09:50,701 INFO] Step 8900/50000; xent: 2.80; lr: 0.0000178;   9 docs/s;  20682 sec
[2024-04-15 23:11:32,882 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.77.bert.pt, number of examples: 2001
[2024-04-15 23:11:46,710 INFO] Step 8950/50000; xent: 2.78; lr: 0.0000179;   9 docs/s;  20798 sec
[2024-04-15 23:13:42,825 INFO] Step 9000/50000; xent: 2.82; lr: 0.0000180;   9 docs/s;  20914 sec
[2024-04-15 23:13:42,835 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_9000.pt
[2024-04-15 23:15:40,272 INFO] Step 9050/50000; xent: 2.77; lr: 0.0000181;   9 docs/s;  21032 sec
[2024-04-15 23:17:37,126 INFO] Step 9100/50000; xent: 2.86; lr: 0.0000182;   9 docs/s;  21149 sec
[2024-04-15 23:18:57,428 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.131.bert.pt, number of examples: 2001
[2024-04-15 23:19:32,471 INFO] Step 9150/50000; xent: 2.88; lr: 0.0000183;   9 docs/s;  21264 sec
[2024-04-15 23:21:28,507 INFO] Step 9200/50000; xent: 2.83; lr: 0.0000184;   9 docs/s;  21380 sec
[2024-04-15 23:23:24,264 INFO] Step 9250/50000; xent: 2.78; lr: 0.0000185;   9 docs/s;  21496 sec
[2024-04-15 23:25:19,382 INFO] Step 9300/50000; xent: 2.78; lr: 0.0000186;   9 docs/s;  21611 sec
[2024-04-15 23:26:18,148 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.7.bert.pt, number of examples: 2001
[2024-04-15 23:27:16,230 INFO] Step 9350/50000; xent: 2.78; lr: 0.0000187;   9 docs/s;  21728 sec
[2024-04-15 23:29:11,862 INFO] Step 9400/50000; xent: 2.83; lr: 0.0000188;   9 docs/s;  21843 sec
[2024-04-15 23:31:08,745 INFO] Step 9450/50000; xent: 2.76; lr: 0.0000189;   9 docs/s;  21960 sec
[2024-04-15 23:33:03,260 INFO] Step 9500/50000; xent: 2.86; lr: 0.0000190;   9 docs/s;  22075 sec
[2024-04-15 23:33:40,097 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.9.bert.pt, number of examples: 1999
[2024-04-15 23:34:59,210 INFO] Step 9550/50000; xent: 2.85; lr: 0.0000191;   9 docs/s;  22191 sec
[2024-04-15 23:36:56,828 INFO] Step 9600/50000; xent: 2.85; lr: 0.0000192;   9 docs/s;  22308 sec
[2024-04-15 23:38:52,938 INFO] Step 9650/50000; xent: 2.89; lr: 0.0000193;   9 docs/s;  22424 sec
[2024-04-15 23:40:47,972 INFO] Step 9700/50000; xent: 2.74; lr: 0.0000194;   9 docs/s;  22539 sec
[2024-04-15 23:41:04,575 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.128.bert.pt, number of examples: 2001
[2024-04-15 23:42:43,384 INFO] Step 9750/50000; xent: 2.78; lr: 0.0000195;   9 docs/s;  22655 sec
[2024-04-15 23:44:41,322 INFO] Step 9800/50000; xent: 2.79; lr: 0.0000196;   9 docs/s;  22773 sec
[2024-04-15 23:46:40,536 INFO] Step 9850/50000; xent: 2.83; lr: 0.0000197;   9 docs/s;  22892 sec
[2024-04-15 23:48:28,774 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.104.bert.pt, number of examples: 2000
[2024-04-15 23:48:36,103 INFO] Step 9900/50000; xent: 2.79; lr: 0.0000198;   9 docs/s;  23008 sec
[2024-04-15 23:50:31,882 INFO] Step 9950/50000; xent: 2.86; lr: 0.0000199;   9 docs/s;  23123 sec
[2024-04-15 23:52:26,630 INFO] Step 10000/50000; xent: 2.76; lr: 0.0000200;   9 docs/s;  23238 sec
[2024-04-15 23:52:26,639 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_10000.pt
[2024-04-15 23:54:26,796 INFO] Step 10050/50000; xent: 2.83; lr: 0.0000200;   9 docs/s;  23358 sec
[2024-04-15 23:55:52,711 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.118.bert.pt, number of examples: 2001
[2024-04-15 23:56:22,350 INFO] Step 10100/50000; xent: 2.76; lr: 0.0000199;   9 docs/s;  23474 sec
[2024-04-15 23:58:19,912 INFO] Step 10150/50000; xent: 2.76; lr: 0.0000199;   9 docs/s;  23591 sec
[2024-04-16 00:00:15,948 INFO] Step 10200/50000; xent: 2.74; lr: 0.0000198;   9 docs/s;  23707 sec
[2024-04-16 00:02:12,996 INFO] Step 10250/50000; xent: 2.79; lr: 0.0000198;   9 docs/s;  23824 sec
[2024-04-16 00:03:16,121 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.8.bert.pt, number of examples: 2000
[2024-04-16 00:04:10,676 INFO] Step 10300/50000; xent: 2.76; lr: 0.0000197;   9 docs/s;  23942 sec
[2024-04-16 00:06:07,596 INFO] Step 10350/50000; xent: 2.75; lr: 0.0000197;   9 docs/s;  24059 sec
[2024-04-16 00:08:06,656 INFO] Step 10400/50000; xent: 2.78; lr: 0.0000196;   9 docs/s;  24178 sec
[2024-04-16 00:10:05,501 INFO] Step 10450/50000; xent: 2.91; lr: 0.0000196;   9 docs/s;  24297 sec
[2024-04-16 00:10:44,374 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.11.bert.pt, number of examples: 1999
[2024-04-16 00:12:01,203 INFO] Step 10500/50000; xent: 2.80; lr: 0.0000195;   9 docs/s;  24413 sec
[2024-04-16 00:13:59,016 INFO] Step 10550/50000; xent: 2.84; lr: 0.0000195;   9 docs/s;  24530 sec
[2024-04-16 00:15:54,251 INFO] Step 10600/50000; xent: 2.87; lr: 0.0000194;   9 docs/s;  24646 sec
[2024-04-16 00:17:50,925 INFO] Step 10650/50000; xent: 2.80; lr: 0.0000194;   9 docs/s;  24762 sec
[2024-04-16 00:18:07,833 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.52.bert.pt, number of examples: 2001
[2024-04-16 00:19:47,545 INFO] Step 10700/50000; xent: 2.80; lr: 0.0000193;   9 docs/s;  24879 sec
[2024-04-16 00:21:44,567 INFO] Step 10750/50000; xent: 2.78; lr: 0.0000193;   9 docs/s;  24996 sec
[2024-04-16 00:23:40,568 INFO] Step 10800/50000; xent: 2.87; lr: 0.0000192;   9 docs/s;  25112 sec
[2024-04-16 00:25:29,829 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.36.bert.pt, number of examples: 2000
[2024-04-16 00:25:36,798 INFO] Step 10850/50000; xent: 2.82; lr: 0.0000192;   9 docs/s;  25228 sec
[2024-04-16 00:27:32,237 INFO] Step 10900/50000; xent: 2.82; lr: 0.0000192;   9 docs/s;  25344 sec
[2024-04-16 00:29:28,419 INFO] Step 10950/50000; xent: 2.76; lr: 0.0000191;   9 docs/s;  25460 sec
[2024-04-16 00:31:24,703 INFO] Step 11000/50000; xent: 2.84; lr: 0.0000191;   9 docs/s;  25576 sec
[2024-04-16 00:31:24,718 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_11000.pt
[2024-04-16 00:32:55,273 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.5.bert.pt, number of examples: 2001
[2024-04-16 00:33:22,913 INFO] Step 11050/50000; xent: 2.80; lr: 0.0000190;   9 docs/s;  25694 sec
[2024-04-16 00:35:19,654 INFO] Step 11100/50000; xent: 2.81; lr: 0.0000190;   9 docs/s;  25811 sec
[2024-04-16 00:37:14,485 INFO] Step 11150/50000; xent: 2.80; lr: 0.0000189;   9 docs/s;  25926 sec
[2024-04-16 00:39:10,841 INFO] Step 11200/50000; xent: 2.79; lr: 0.0000189;   9 docs/s;  26042 sec
[2024-04-16 00:40:19,955 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.33.bert.pt, number of examples: 1999
[2024-04-16 00:41:10,351 INFO] Step 11250/50000; xent: 2.77; lr: 0.0000189;   9 docs/s;  26162 sec
[2024-04-16 00:43:05,756 INFO] Step 11300/50000; xent: 2.78; lr: 0.0000188;   9 docs/s;  26277 sec
[2024-04-16 00:45:02,669 INFO] Step 11350/50000; xent: 2.76; lr: 0.0000188;   9 docs/s;  26394 sec
[2024-04-16 00:46:57,784 INFO] Step 11400/50000; xent: 2.82; lr: 0.0000187;   9 docs/s;  26509 sec
[2024-04-16 00:47:39,403 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.121.bert.pt, number of examples: 2001
[2024-04-16 00:48:52,820 INFO] Step 11450/50000; xent: 2.69; lr: 0.0000187;   9 docs/s;  26624 sec
[2024-04-16 00:50:50,756 INFO] Step 11500/50000; xent: 2.79; lr: 0.0000187;   9 docs/s;  26742 sec
[2024-04-16 00:52:48,367 INFO] Step 11550/50000; xent: 2.79; lr: 0.0000186;   9 docs/s;  26860 sec
[2024-04-16 00:54:43,744 INFO] Step 11600/50000; xent: 2.74; lr: 0.0000186;   9 docs/s;  26975 sec
[2024-04-16 00:55:04,759 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.136.bert.pt, number of examples: 2001
[2024-04-16 00:56:41,014 INFO] Step 11650/50000; xent: 2.74; lr: 0.0000185;   9 docs/s;  27092 sec
[2024-04-16 00:58:37,192 INFO] Step 11700/50000; xent: 2.81; lr: 0.0000185;   9 docs/s;  27209 sec
[2024-04-16 01:00:33,316 INFO] Step 11750/50000; xent: 2.73; lr: 0.0000185;   9 docs/s;  27325 sec
[2024-04-16 01:02:26,390 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.123.bert.pt, number of examples: 2001
[2024-04-16 01:02:28,701 INFO] Step 11800/50000; xent: 2.91; lr: 0.0000184;   9 docs/s;  27440 sec
[2024-04-16 01:04:25,906 INFO] Step 11850/50000; xent: 2.80; lr: 0.0000184;   9 docs/s;  27557 sec
[2024-04-16 01:06:21,311 INFO] Step 11900/50000; xent: 2.76; lr: 0.0000183;   9 docs/s;  27673 sec
[2024-04-16 01:08:16,516 INFO] Step 11950/50000; xent: 2.78; lr: 0.0000183;   9 docs/s;  27788 sec
[2024-04-16 01:09:49,056 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.71.bert.pt, number of examples: 1999
[2024-04-16 01:10:12,784 INFO] Step 12000/50000; xent: 2.76; lr: 0.0000183;   9 docs/s;  27904 sec
[2024-04-16 01:10:12,793 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_12000.pt
[2024-04-16 01:12:11,836 INFO] Step 12050/50000; xent: 2.78; lr: 0.0000182;   9 docs/s;  28023 sec
[2024-04-16 01:14:07,439 INFO] Step 12100/50000; xent: 2.85; lr: 0.0000182;   9 docs/s;  28139 sec
[2024-04-16 01:16:01,694 INFO] Step 12150/50000; xent: 2.81; lr: 0.0000181;   9 docs/s;  28253 sec
[2024-04-16 01:17:12,840 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.45.bert.pt, number of examples: 2001
[2024-04-16 01:17:58,876 INFO] Step 12200/50000; xent: 2.80; lr: 0.0000181;   9 docs/s;  28370 sec
[2024-04-16 01:19:55,684 INFO] Step 12250/50000; xent: 2.80; lr: 0.0000181;   9 docs/s;  28487 sec
[2024-04-16 01:21:51,399 INFO] Step 12300/50000; xent: 2.77; lr: 0.0000180;   9 docs/s;  28603 sec
[2024-04-16 01:23:46,458 INFO] Step 12350/50000; xent: 2.79; lr: 0.0000180;   9 docs/s;  28718 sec
[2024-04-16 01:24:37,254 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.134.bert.pt, number of examples: 2001
[2024-04-16 01:25:41,340 INFO] Step 12400/50000; xent: 2.74; lr: 0.0000180;   9 docs/s;  28833 sec
[2024-04-16 01:27:36,076 INFO] Step 12450/50000; xent: 2.78; lr: 0.0000179;   9 docs/s;  28948 sec
[2024-04-16 01:29:31,222 INFO] Step 12500/50000; xent: 2.86; lr: 0.0000179;   9 docs/s;  29063 sec
[2024-04-16 01:31:27,752 INFO] Step 12550/50000; xent: 2.74; lr: 0.0000179;   9 docs/s;  29179 sec
[2024-04-16 01:31:56,010 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.22.bert.pt, number of examples: 1999
[2024-04-16 01:33:24,253 INFO] Step 12600/50000; xent: 2.83; lr: 0.0000178;   9 docs/s;  29296 sec
[2024-04-16 01:35:20,590 INFO] Step 12650/50000; xent: 2.72; lr: 0.0000178;   9 docs/s;  29412 sec
[2024-04-16 01:37:14,868 INFO] Step 12700/50000; xent: 2.77; lr: 0.0000177;   9 docs/s;  29526 sec
[2024-04-16 01:39:12,726 INFO] Step 12750/50000; xent: 2.78; lr: 0.0000177;   9 docs/s;  29644 sec
[2024-04-16 01:39:17,512 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.63.bert.pt, number of examples: 2001
[2024-04-16 01:41:08,290 INFO] Step 12800/50000; xent: 2.80; lr: 0.0000177;   9 docs/s;  29760 sec
[2024-04-16 01:43:06,423 INFO] Step 12850/50000; xent: 2.74; lr: 0.0000176;   9 docs/s;  29878 sec
[2024-04-16 01:45:03,313 INFO] Step 12900/50000; xent: 2.83; lr: 0.0000176;   9 docs/s;  29995 sec
[2024-04-16 01:46:36,759 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.141.bert.pt, number of examples: 1999
[2024-04-16 01:46:58,068 INFO] Step 12950/50000; xent: 2.77; lr: 0.0000176;   9 docs/s;  30110 sec
[2024-04-16 01:48:55,248 INFO] Step 13000/50000; xent: 2.71; lr: 0.0000175;   9 docs/s;  30227 sec
[2024-04-16 01:48:55,258 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_13000.pt
[2024-04-16 01:50:54,193 INFO] Step 13050/50000; xent: 2.79; lr: 0.0000175;   9 docs/s;  30346 sec
[2024-04-16 01:52:49,317 INFO] Step 13100/50000; xent: 2.78; lr: 0.0000175;   9 docs/s;  30461 sec
[2024-04-16 01:54:01,487 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.17.bert.pt, number of examples: 2000
[2024-04-16 01:54:45,241 INFO] Step 13150/50000; xent: 2.79; lr: 0.0000174;   9 docs/s;  30577 sec
[2024-04-16 01:56:41,458 INFO] Step 13200/50000; xent: 2.72; lr: 0.0000174;   9 docs/s;  30693 sec
[2024-04-16 01:58:37,917 INFO] Step 13250/50000; xent: 2.80; lr: 0.0000174;   9 docs/s;  30809 sec
[2024-04-16 02:00:31,646 INFO] Step 13300/50000; xent: 2.84; lr: 0.0000173;   9 docs/s;  30923 sec
[2024-04-16 02:01:21,768 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.116.bert.pt, number of examples: 2000
[2024-04-16 02:02:27,020 INFO] Step 13350/50000; xent: 2.80; lr: 0.0000173;   9 docs/s;  31038 sec
[2024-04-16 02:04:23,073 INFO] Step 13400/50000; xent: 2.73; lr: 0.0000173;   9 docs/s;  31155 sec
[2024-04-16 02:06:20,277 INFO] Step 13450/50000; xent: 2.80; lr: 0.0000172;   9 docs/s;  31272 sec
[2024-04-16 02:08:15,682 INFO] Step 13500/50000; xent: 2.78; lr: 0.0000172;   9 docs/s;  31387 sec
[2024-04-16 02:08:44,110 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.79.bert.pt, number of examples: 1999
[2024-04-16 02:10:11,557 INFO] Step 13550/50000; xent: 2.76; lr: 0.0000172;   9 docs/s;  31503 sec
[2024-04-16 02:12:08,408 INFO] Step 13600/50000; xent: 2.83; lr: 0.0000171;   9 docs/s;  31620 sec
[2024-04-16 02:14:06,642 INFO] Step 13650/50000; xent: 2.79; lr: 0.0000171;   9 docs/s;  31738 sec
[2024-04-16 02:16:03,333 INFO] Step 13700/50000; xent: 2.80; lr: 0.0000171;   9 docs/s;  31855 sec
[2024-04-16 02:16:10,445 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.2.bert.pt, number of examples: 2001
[2024-04-16 02:18:02,243 INFO] Step 13750/50000; xent: 2.80; lr: 0.0000171;   9 docs/s;  31974 sec
[2024-04-16 02:19:56,215 INFO] Step 13800/50000; xent: 2.71; lr: 0.0000170;   9 docs/s;  32088 sec
[2024-04-16 02:21:51,044 INFO] Step 13850/50000; xent: 2.75; lr: 0.0000170;   9 docs/s;  32202 sec
[2024-04-16 02:23:30,876 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.107.bert.pt, number of examples: 1999
[2024-04-16 02:23:47,032 INFO] Step 13900/50000; xent: 2.72; lr: 0.0000170;   9 docs/s;  32318 sec
[2024-04-16 02:25:44,508 INFO] Step 13950/50000; xent: 2.74; lr: 0.0000169;   9 docs/s;  32436 sec
[2024-04-16 02:27:40,998 INFO] Step 14000/50000; xent: 2.82; lr: 0.0000169;   9 docs/s;  32552 sec
[2024-04-16 02:27:41,009 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_14000.pt
[2024-04-16 02:29:39,657 INFO] Step 14050/50000; xent: 2.80; lr: 0.0000169;   9 docs/s;  32671 sec
[2024-04-16 02:30:54,578 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.125.bert.pt, number of examples: 2000
[2024-04-16 02:31:36,970 INFO] Step 14100/50000; xent: 2.78; lr: 0.0000168;   9 docs/s;  32788 sec
[2024-04-16 02:33:34,390 INFO] Step 14150/50000; xent: 2.79; lr: 0.0000168;   9 docs/s;  32906 sec
[2024-04-16 02:35:29,647 INFO] Step 14200/50000; xent: 2.87; lr: 0.0000168;   9 docs/s;  33021 sec
[2024-04-16 02:37:23,531 INFO] Step 14250/50000; xent: 2.74; lr: 0.0000168;   9 docs/s;  33135 sec
[2024-04-16 02:38:16,084 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.142.bert.pt, number of examples: 2000
[2024-04-16 02:39:20,792 INFO] Step 14300/50000; xent: 2.79; lr: 0.0000167;   9 docs/s;  33252 sec
[2024-04-16 02:41:17,556 INFO] Step 14350/50000; xent: 2.75; lr: 0.0000167;   9 docs/s;  33369 sec
[2024-04-16 02:43:11,908 INFO] Step 14400/50000; xent: 2.78; lr: 0.0000167;   9 docs/s;  33483 sec
[2024-04-16 02:45:07,302 INFO] Step 14450/50000; xent: 2.78; lr: 0.0000166;   9 docs/s;  33599 sec
[2024-04-16 02:45:37,011 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.66.bert.pt, number of examples: 2001
[2024-04-16 02:47:02,825 INFO] Step 14500/50000; xent: 2.74; lr: 0.0000166;   9 docs/s;  33714 sec
[2024-04-16 02:48:58,704 INFO] Step 14550/50000; xent: 2.77; lr: 0.0000166;   9 docs/s;  33830 sec
[2024-04-16 02:50:55,427 INFO] Step 14600/50000; xent: 2.79; lr: 0.0000166;   9 docs/s;  33947 sec
[2024-04-16 02:52:49,849 INFO] Step 14650/50000; xent: 2.68; lr: 0.0000165;   9 docs/s;  34061 sec
[2024-04-16 02:52:56,996 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.44.bert.pt, number of examples: 1998
[2024-04-16 02:54:46,877 INFO] Step 14700/50000; xent: 2.78; lr: 0.0000165;   9 docs/s;  34178 sec
[2024-04-16 02:56:42,301 INFO] Step 14750/50000; xent: 2.77; lr: 0.0000165;   9 docs/s;  34294 sec
[2024-04-16 02:58:36,892 INFO] Step 14800/50000; xent: 2.79; lr: 0.0000164;   9 docs/s;  34408 sec
[2024-04-16 03:00:20,220 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.85.bert.pt, number of examples: 2001
[2024-04-16 03:00:36,419 INFO] Step 14850/50000; xent: 2.74; lr: 0.0000164;   9 docs/s;  34528 sec
[2024-04-16 03:02:31,954 INFO] Step 14900/50000; xent: 2.75; lr: 0.0000164;   9 docs/s;  34643 sec
[2024-04-16 03:04:29,491 INFO] Step 14950/50000; xent: 2.81; lr: 0.0000164;   9 docs/s;  34761 sec
[2024-04-16 03:06:24,893 INFO] Step 15000/50000; xent: 2.76; lr: 0.0000163;   9 docs/s;  34876 sec
[2024-04-16 03:06:24,902 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_15000.pt
[2024-04-16 03:07:47,207 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.99.bert.pt, number of examples: 2000
[2024-04-16 03:08:24,107 INFO] Step 15050/50000; xent: 2.74; lr: 0.0000163;   9 docs/s;  34996 sec
[2024-04-16 03:10:22,459 INFO] Step 15100/50000; xent: 2.84; lr: 0.0000163;   9 docs/s;  35114 sec
[2024-04-16 03:12:17,855 INFO] Step 15150/50000; xent: 2.74; lr: 0.0000162;   9 docs/s;  35229 sec
[2024-04-16 03:14:13,034 INFO] Step 15200/50000; xent: 2.77; lr: 0.0000162;   9 docs/s;  35344 sec
[2024-04-16 03:15:06,162 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.110.bert.pt, number of examples: 2000
[2024-04-16 03:16:08,821 INFO] Step 15250/50000; xent: 2.78; lr: 0.0000162;   9 docs/s;  35460 sec
[2024-04-16 03:18:05,140 INFO] Step 15300/50000; xent: 2.77; lr: 0.0000162;   9 docs/s;  35577 sec
[2024-04-16 03:19:59,848 INFO] Step 15350/50000; xent: 2.84; lr: 0.0000161;   9 docs/s;  35691 sec
[2024-04-16 03:21:54,672 INFO] Step 15400/50000; xent: 2.75; lr: 0.0000161;   9 docs/s;  35806 sec
[2024-04-16 03:22:22,699 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.24.bert.pt, number of examples: 2001
[2024-04-16 03:23:51,124 INFO] Step 15450/50000; xent: 2.70; lr: 0.0000161;   9 docs/s;  35923 sec
[2024-04-16 03:25:45,334 INFO] Step 15500/50000; xent: 2.75; lr: 0.0000161;   9 docs/s;  36037 sec
[2024-04-16 03:27:42,577 INFO] Step 15550/50000; xent: 2.76; lr: 0.0000160;   9 docs/s;  36154 sec
[2024-04-16 03:29:37,219 INFO] Step 15600/50000; xent: 2.73; lr: 0.0000160;   9 docs/s;  36269 sec
[2024-04-16 03:29:46,846 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.53.bert.pt, number of examples: 1999
[2024-04-16 03:31:34,812 INFO] Step 15650/50000; xent: 2.81; lr: 0.0000160;   9 docs/s;  36386 sec
[2024-04-16 03:33:30,078 INFO] Step 15700/50000; xent: 2.80; lr: 0.0000160;   9 docs/s;  36502 sec
[2024-04-16 03:35:25,776 INFO] Step 15750/50000; xent: 2.75; lr: 0.0000159;   9 docs/s;  36617 sec
[2024-04-16 03:37:07,851 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.28.bert.pt, number of examples: 2000
[2024-04-16 03:37:21,534 INFO] Step 15800/50000; xent: 2.77; lr: 0.0000159;   9 docs/s;  36733 sec
[2024-04-16 03:39:17,909 INFO] Step 15850/50000; xent: 2.74; lr: 0.0000159;   9 docs/s;  36849 sec
[2024-04-16 03:41:15,534 INFO] Step 15900/50000; xent: 2.78; lr: 0.0000159;   9 docs/s;  36967 sec
[2024-04-16 03:43:12,792 INFO] Step 15950/50000; xent: 2.80; lr: 0.0000158;   9 docs/s;  37084 sec
[2024-04-16 03:44:29,015 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.14.bert.pt, number of examples: 1998
[2024-04-16 03:45:06,271 INFO] Step 16000/50000; xent: 2.75; lr: 0.0000158;   9 docs/s;  37198 sec
[2024-04-16 03:45:06,281 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_16000.pt
[2024-04-16 03:47:05,071 INFO] Step 16050/50000; xent: 2.79; lr: 0.0000158;   9 docs/s;  37317 sec
[2024-04-16 03:49:00,239 INFO] Step 16100/50000; xent: 2.68; lr: 0.0000158;   9 docs/s;  37432 sec
[2024-04-16 03:50:56,060 INFO] Step 16150/50000; xent: 2.79; lr: 0.0000157;   9 docs/s;  37548 sec
[2024-04-16 03:51:54,482 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.48.bert.pt, number of examples: 2000
[2024-04-16 03:52:52,548 INFO] Step 16200/50000; xent: 2.74; lr: 0.0000157;   9 docs/s;  37664 sec
[2024-04-16 03:54:49,346 INFO] Step 16250/50000; xent: 2.79; lr: 0.0000157;   9 docs/s;  37781 sec
[2024-04-16 03:56:44,217 INFO] Step 16300/50000; xent: 2.68; lr: 0.0000157;   9 docs/s;  37896 sec
[2024-04-16 03:58:40,813 INFO] Step 16350/50000; xent: 2.75; lr: 0.0000156;   9 docs/s;  38012 sec
[2024-04-16 03:59:18,390 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.19.bert.pt, number of examples: 2000
[2024-04-16 04:00:37,538 INFO] Step 16400/50000; xent: 2.76; lr: 0.0000156;   9 docs/s;  38129 sec
[2024-04-16 04:02:35,374 INFO] Step 16450/50000; xent: 2.81; lr: 0.0000156;   9 docs/s;  38247 sec
[2024-04-16 04:04:31,894 INFO] Step 16500/50000; xent: 2.81; lr: 0.0000156;   9 docs/s;  38363 sec
[2024-04-16 04:06:30,187 INFO] Step 16550/50000; xent: 2.77; lr: 0.0000155;   9 docs/s;  38482 sec
[2024-04-16 04:06:45,047 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.87.bert.pt, number of examples: 1999
[2024-04-16 04:08:32,174 INFO] Step 16600/50000; xent: 2.80; lr: 0.0000155;   9 docs/s;  38604 sec
[2024-04-16 04:10:29,518 INFO] Step 16650/50000; xent: 2.77; lr: 0.0000155;   9 docs/s;  38721 sec
[2024-04-16 04:12:28,817 INFO] Step 16700/50000; xent: 2.74; lr: 0.0000155;   9 docs/s;  38840 sec
[2024-04-16 04:14:16,785 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.20.bert.pt, number of examples: 2000
[2024-04-16 04:14:26,498 INFO] Step 16750/50000; xent: 2.69; lr: 0.0000155;   9 docs/s;  38958 sec
[2024-04-16 04:16:25,181 INFO] Step 16800/50000; xent: 2.71; lr: 0.0000154;   9 docs/s;  39077 sec
[2024-04-16 04:18:25,604 INFO] Step 16850/50000; xent: 2.81; lr: 0.0000154;   9 docs/s;  39197 sec
[2024-04-16 04:20:24,512 INFO] Step 16900/50000; xent: 2.84; lr: 0.0000154;   9 docs/s;  39316 sec
[2024-04-16 04:21:49,899 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.67.bert.pt, number of examples: 1999
[2024-04-16 04:22:22,631 INFO] Step 16950/50000; xent: 2.72; lr: 0.0000154;   9 docs/s;  39434 sec
[2024-04-16 04:24:22,406 INFO] Step 17000/50000; xent: 2.77; lr: 0.0000153;   9 docs/s;  39554 sec
[2024-04-16 04:24:22,416 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_17000.pt
[2024-04-16 04:26:23,893 INFO] Step 17050/50000; xent: 2.76; lr: 0.0000153;   9 docs/s;  39675 sec
[2024-04-16 04:28:19,128 INFO] Step 17100/50000; xent: 2.73; lr: 0.0000153;   9 docs/s;  39791 sec
[2024-04-16 04:29:20,718 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.26.bert.pt, number of examples: 2000
[2024-04-16 04:30:13,546 INFO] Step 17150/50000; xent: 2.75; lr: 0.0000153;   9 docs/s;  39905 sec
[2024-04-16 04:32:10,086 INFO] Step 17200/50000; xent: 2.80; lr: 0.0000152;   9 docs/s;  40022 sec
[2024-04-16 04:34:06,619 INFO] Step 17250/50000; xent: 2.73; lr: 0.0000152;   9 docs/s;  40138 sec
[2024-04-16 04:36:02,618 INFO] Step 17300/50000; xent: 2.79; lr: 0.0000152;   9 docs/s;  40254 sec
[2024-04-16 04:36:40,212 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.23.bert.pt, number of examples: 2001
[2024-04-16 04:37:59,131 INFO] Step 17350/50000; xent: 2.72; lr: 0.0000152;   9 docs/s;  40371 sec
[2024-04-16 04:39:56,610 INFO] Step 17400/50000; xent: 2.68; lr: 0.0000152;   9 docs/s;  40488 sec
[2024-04-16 04:41:53,480 INFO] Step 17450/50000; xent: 2.75; lr: 0.0000151;   9 docs/s;  40605 sec
[2024-04-16 04:43:49,523 INFO] Step 17500/50000; xent: 2.74; lr: 0.0000151;   9 docs/s;  40721 sec
[2024-04-16 04:44:05,550 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.127.bert.pt, number of examples: 2000
[2024-04-16 04:45:45,578 INFO] Step 17550/50000; xent: 2.74; lr: 0.0000151;   9 docs/s;  40837 sec
[2024-04-16 04:47:41,532 INFO] Step 17600/50000; xent: 2.81; lr: 0.0000151;   9 docs/s;  40953 sec
[2024-04-16 04:49:37,539 INFO] Step 17650/50000; xent: 2.72; lr: 0.0000151;   9 docs/s;  41069 sec
[2024-04-16 04:51:26,871 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.98.bert.pt, number of examples: 2000
[2024-04-16 04:51:33,804 INFO] Step 17700/50000; xent: 2.79; lr: 0.0000150;   9 docs/s;  41185 sec
[2024-04-16 04:53:29,921 INFO] Step 17750/50000; xent: 2.75; lr: 0.0000150;   9 docs/s;  41301 sec
[2024-04-16 04:55:27,489 INFO] Step 17800/50000; xent: 2.74; lr: 0.0000150;   9 docs/s;  41419 sec
[2024-04-16 04:57:23,183 INFO] Step 17850/50000; xent: 2.81; lr: 0.0000150;   9 docs/s;  41535 sec
[2024-04-16 04:58:50,384 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.74.bert.pt, number of examples: 2000
[2024-04-16 04:59:21,008 INFO] Step 17900/50000; xent: 2.84; lr: 0.0000149;   9 docs/s;  41652 sec
[2024-04-16 05:01:15,813 INFO] Step 17950/50000; xent: 2.67; lr: 0.0000149;   9 docs/s;  41767 sec
[2024-04-16 05:03:11,511 INFO] Step 18000/50000; xent: 2.75; lr: 0.0000149;   9 docs/s;  41883 sec
[2024-04-16 05:03:11,524 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_18000.pt
[2024-04-16 05:05:11,752 INFO] Step 18050/50000; xent: 2.78; lr: 0.0000149;   9 docs/s;  42003 sec
[2024-04-16 05:06:16,693 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.65.bert.pt, number of examples: 2000
[2024-04-16 05:07:07,206 INFO] Step 18100/50000; xent: 2.72; lr: 0.0000149;   9 docs/s;  42119 sec
[2024-04-16 05:09:03,472 INFO] Step 18150/50000; xent: 2.64; lr: 0.0000148;   9 docs/s;  42235 sec
[2024-04-16 05:10:58,411 INFO] Step 18200/50000; xent: 2.76; lr: 0.0000148;   9 docs/s;  42350 sec
[2024-04-16 05:12:55,387 INFO] Step 18250/50000; xent: 2.83; lr: 0.0000148;   9 docs/s;  42467 sec
[2024-04-16 05:13:39,511 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.119.bert.pt, number of examples: 2000
[2024-04-16 05:14:53,687 INFO] Step 18300/50000; xent: 2.73; lr: 0.0000148;   9 docs/s;  42585 sec
[2024-04-16 05:16:50,980 INFO] Step 18350/50000; xent: 2.76; lr: 0.0000148;   9 docs/s;  42702 sec
[2024-04-16 05:18:46,855 INFO] Step 18400/50000; xent: 2.70; lr: 0.0000147;   9 docs/s;  42818 sec
[2024-04-16 05:20:43,311 INFO] Step 18450/50000; xent: 2.74; lr: 0.0000147;   9 docs/s;  42935 sec
[2024-04-16 05:21:03,609 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.60.bert.pt, number of examples: 2000
[2024-04-16 05:22:39,557 INFO] Step 18500/50000; xent: 2.80; lr: 0.0000147;   9 docs/s;  43051 sec
[2024-04-16 05:24:35,420 INFO] Step 18550/50000; xent: 2.74; lr: 0.0000147;   9 docs/s;  43167 sec
[2024-04-16 05:26:29,983 INFO] Step 18600/50000; xent: 2.75; lr: 0.0000147;   9 docs/s;  43281 sec
[2024-04-16 05:28:23,623 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.55.bert.pt, number of examples: 1999
[2024-04-16 05:28:25,933 INFO] Step 18650/50000; xent: 2.74; lr: 0.0000146;   9 docs/s;  43397 sec
[2024-04-16 05:30:20,880 INFO] Step 18700/50000; xent: 2.75; lr: 0.0000146;   9 docs/s;  43512 sec
[2024-04-16 05:32:17,293 INFO] Step 18750/50000; xent: 2.68; lr: 0.0000146;   9 docs/s;  43629 sec
[2024-04-16 05:34:14,106 INFO] Step 18800/50000; xent: 2.83; lr: 0.0000146;   9 docs/s;  43746 sec
[2024-04-16 05:35:46,261 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.103.bert.pt, number of examples: 2000
[2024-04-16 05:36:11,528 INFO] Step 18850/50000; xent: 2.69; lr: 0.0000146;   9 docs/s;  43863 sec
[2024-04-16 05:38:07,284 INFO] Step 18900/50000; xent: 2.69; lr: 0.0000145;   9 docs/s;  43979 sec
[2024-04-16 05:40:04,653 INFO] Step 18950/50000; xent: 2.71; lr: 0.0000145;   9 docs/s;  44096 sec
[2024-04-16 05:42:00,398 INFO] Step 19000/50000; xent: 2.68; lr: 0.0000145;   9 docs/s;  44212 sec
[2024-04-16 05:42:00,408 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_19000.pt
[2024-04-16 05:43:10,966 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.1.bert.pt, number of examples: 2001
[2024-04-16 05:44:00,145 INFO] Step 19050/50000; xent: 2.84; lr: 0.0000145;   9 docs/s;  44332 sec
[2024-04-16 05:45:55,731 INFO] Step 19100/50000; xent: 2.76; lr: 0.0000145;   9 docs/s;  44447 sec
[2024-04-16 05:47:51,143 INFO] Step 19150/50000; xent: 2.74; lr: 0.0000145;   9 docs/s;  44563 sec
[2024-04-16 05:49:49,086 INFO] Step 19200/50000; xent: 2.72; lr: 0.0000144;   9 docs/s;  44681 sec
[2024-04-16 05:50:33,480 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.30.bert.pt, number of examples: 1996
[2024-04-16 05:51:46,351 INFO] Step 19250/50000; xent: 2.78; lr: 0.0000144;   9 docs/s;  44798 sec
[2024-04-16 05:53:41,968 INFO] Step 19300/50000; xent: 2.73; lr: 0.0000144;   9 docs/s;  44913 sec
[2024-04-16 05:55:39,714 INFO] Step 19350/50000; xent: 2.79; lr: 0.0000144;   9 docs/s;  45031 sec
[2024-04-16 05:57:34,688 INFO] Step 19400/50000; xent: 2.79; lr: 0.0000144;   9 docs/s;  45146 sec
[2024-04-16 05:57:58,078 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.10.bert.pt, number of examples: 2001
[2024-04-16 05:59:30,597 INFO] Step 19450/50000; xent: 2.78; lr: 0.0000143;   9 docs/s;  45262 sec
[2024-04-16 06:01:27,707 INFO] Step 19500/50000; xent: 2.79; lr: 0.0000143;   9 docs/s;  45379 sec
[2024-04-16 06:03:23,972 INFO] Step 19550/50000; xent: 2.71; lr: 0.0000143;   9 docs/s;  45495 sec
[2024-04-16 06:05:20,909 INFO] Step 19600/50000; xent: 2.80; lr: 0.0000143;   9 docs/s;  45612 sec
[2024-04-16 06:05:21,149 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.89.bert.pt, number of examples: 2001
[2024-04-16 06:07:17,525 INFO] Step 19650/50000; xent: 2.76; lr: 0.0000143;   9 docs/s;  45729 sec
[2024-04-16 06:09:14,021 INFO] Step 19700/50000; xent: 2.72; lr: 0.0000142;   9 docs/s;  45845 sec
[2024-04-16 06:11:09,376 INFO] Step 19750/50000; xent: 2.66; lr: 0.0000142;   9 docs/s;  45961 sec
[2024-04-16 06:12:41,189 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.112.bert.pt, number of examples: 1999
[2024-04-16 06:13:04,823 INFO] Step 19800/50000; xent: 2.82; lr: 0.0000142;   9 docs/s;  46076 sec
[2024-04-16 06:15:01,279 INFO] Step 19850/50000; xent: 2.83; lr: 0.0000142;   9 docs/s;  46193 sec
[2024-04-16 06:16:57,175 INFO] Step 19900/50000; xent: 2.78; lr: 0.0000142;   9 docs/s;  46309 sec
[2024-04-16 06:18:54,684 INFO] Step 19950/50000; xent: 2.80; lr: 0.0000142;   9 docs/s;  46426 sec
[2024-04-16 06:20:02,130 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.102.bert.pt, number of examples: 1999
[2024-04-16 06:20:50,443 INFO] Step 20000/50000; xent: 2.80; lr: 0.0000141;   9 docs/s;  46542 sec
[2024-04-16 06:20:50,454 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_20000.pt
[2024-04-16 06:22:48,726 INFO] Step 20050/50000; xent: 2.83; lr: 0.0000141;   9 docs/s;  46660 sec
[2024-04-16 06:24:47,205 INFO] Step 20100/50000; xent: 2.77; lr: 0.0000141;   9 docs/s;  46779 sec
[2024-04-16 06:26:43,837 INFO] Step 20150/50000; xent: 2.81; lr: 0.0000141;   9 docs/s;  46895 sec
[2024-04-16 06:27:29,528 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.41.bert.pt, number of examples: 2000
[2024-04-16 06:28:37,798 INFO] Step 20200/50000; xent: 2.76; lr: 0.0000141;   9 docs/s;  47009 sec
[2024-04-16 06:30:33,411 INFO] Step 20250/50000; xent: 2.75; lr: 0.0000141;   9 docs/s;  47125 sec
[2024-04-16 06:32:29,887 INFO] Step 20300/50000; xent: 2.73; lr: 0.0000140;   9 docs/s;  47241 sec
[2024-04-16 06:34:26,127 INFO] Step 20350/50000; xent: 2.77; lr: 0.0000140;   9 docs/s;  47358 sec
[2024-04-16 06:34:51,153 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.122.bert.pt, number of examples: 2000
[2024-04-16 06:36:21,718 INFO] Step 20400/50000; xent: 2.71; lr: 0.0000140;   9 docs/s;  47473 sec
[2024-04-16 06:38:19,000 INFO] Step 20450/50000; xent: 2.79; lr: 0.0000140;   9 docs/s;  47590 sec
[2024-04-16 06:40:15,323 INFO] Step 20500/50000; xent: 2.77; lr: 0.0000140;   9 docs/s;  47707 sec
[2024-04-16 06:42:10,847 INFO] Step 20550/50000; xent: 2.75; lr: 0.0000140;   9 docs/s;  47822 sec
[2024-04-16 06:42:11,251 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.133.bert.pt, number of examples: 1999
[2024-04-16 06:44:06,534 INFO] Step 20600/50000; xent: 2.76; lr: 0.0000139;   9 docs/s;  47938 sec
[2024-04-16 06:46:04,436 INFO] Step 20650/50000; xent: 2.73; lr: 0.0000139;   9 docs/s;  48056 sec
[2024-04-16 06:47:59,909 INFO] Step 20700/50000; xent: 2.80; lr: 0.0000139;   9 docs/s;  48171 sec
[2024-04-16 06:49:32,202 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.68.bert.pt, number of examples: 1999
[2024-04-16 06:49:55,128 INFO] Step 20750/50000; xent: 2.75; lr: 0.0000139;   9 docs/s;  48287 sec
[2024-04-16 06:51:50,664 INFO] Step 20800/50000; xent: 2.76; lr: 0.0000139;   9 docs/s;  48402 sec
[2024-04-16 06:53:48,466 INFO] Step 20850/50000; xent: 2.75; lr: 0.0000139;   9 docs/s;  48520 sec
[2024-04-16 06:55:45,087 INFO] Step 20900/50000; xent: 2.71; lr: 0.0000138;   9 docs/s;  48637 sec
[2024-04-16 06:56:55,452 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.108.bert.pt, number of examples: 2000
[2024-04-16 06:57:41,760 INFO] Step 20950/50000; xent: 2.73; lr: 0.0000138;   9 docs/s;  48753 sec
[2024-04-16 06:59:37,877 INFO] Step 21000/50000; xent: 2.66; lr: 0.0000138;   9 docs/s;  48869 sec
[2024-04-16 06:59:37,887 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_21000.pt
[2024-04-16 07:01:36,729 INFO] Step 21050/50000; xent: 2.73; lr: 0.0000138;   9 docs/s;  48988 sec
[2024-04-16 07:03:32,214 INFO] Step 21100/50000; xent: 2.82; lr: 0.0000138;   9 docs/s;  49104 sec
[2024-04-16 07:04:20,574 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.61.bert.pt, number of examples: 2001
[2024-04-16 07:05:27,434 INFO] Step 21150/50000; xent: 2.74; lr: 0.0000138;   9 docs/s;  49219 sec
[2024-04-16 07:07:23,911 INFO] Step 21200/50000; xent: 2.74; lr: 0.0000137;   9 docs/s;  49335 sec
[2024-04-16 07:09:19,386 INFO] Step 21250/50000; xent: 2.70; lr: 0.0000137;   9 docs/s;  49451 sec
[2024-04-16 07:11:14,320 INFO] Step 21300/50000; xent: 2.83; lr: 0.0000137;   9 docs/s;  49566 sec
[2024-04-16 07:11:40,098 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.3.bert.pt, number of examples: 2001
[2024-04-16 07:13:10,337 INFO] Step 21350/50000; xent: 2.72; lr: 0.0000137;   9 docs/s;  49682 sec
[2024-04-16 07:15:05,261 INFO] Step 21400/50000; xent: 2.75; lr: 0.0000137;   9 docs/s;  49797 sec
[2024-04-16 07:17:01,651 INFO] Step 21450/50000; xent: 2.63; lr: 0.0000137;   9 docs/s;  49913 sec
[2024-04-16 07:18:57,908 INFO] Step 21500/50000; xent: 2.77; lr: 0.0000136;   9 docs/s;  50029 sec
[2024-04-16 07:19:02,835 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.90.bert.pt, number of examples: 2000
[2024-04-16 07:20:54,807 INFO] Step 21550/50000; xent: 2.76; lr: 0.0000136;   9 docs/s;  50146 sec
[2024-04-16 07:22:52,051 INFO] Step 21600/50000; xent: 2.67; lr: 0.0000136;   9 docs/s;  50263 sec
[2024-04-16 07:24:48,767 INFO] Step 21650/50000; xent: 2.77; lr: 0.0000136;   9 docs/s;  50380 sec
[2024-04-16 07:26:29,919 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.50.bert.pt, number of examples: 2001
[2024-04-16 07:26:48,625 INFO] Step 21700/50000; xent: 2.70; lr: 0.0000136;   9 docs/s;  50500 sec
[2024-04-16 07:28:45,156 INFO] Step 21750/50000; xent: 2.77; lr: 0.0000136;   9 docs/s;  50617 sec
[2024-04-16 07:30:41,914 INFO] Step 21800/50000; xent: 2.74; lr: 0.0000135;   9 docs/s;  50733 sec
[2024-04-16 07:32:38,877 INFO] Step 21850/50000; xent: 2.70; lr: 0.0000135;   9 docs/s;  50850 sec
[2024-04-16 07:33:53,287 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.140.bert.pt, number of examples: 2000
[2024-04-16 07:34:34,906 INFO] Step 21900/50000; xent: 2.78; lr: 0.0000135;   9 docs/s;  50966 sec
[2024-04-16 07:36:32,979 INFO] Step 21950/50000; xent: 2.78; lr: 0.0000135;   9 docs/s;  51084 sec
[2024-04-16 07:38:29,305 INFO] Step 22000/50000; xent: 2.80; lr: 0.0000135;   9 docs/s;  51201 sec
[2024-04-16 07:38:29,314 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_22000.pt
[2024-04-16 07:40:28,150 INFO] Step 22050/50000; xent: 2.74; lr: 0.0000135;   9 docs/s;  51320 sec
[2024-04-16 07:41:21,237 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.18.bert.pt, number of examples: 1998
[2024-04-16 07:42:23,222 INFO] Step 22100/50000; xent: 2.75; lr: 0.0000135;   9 docs/s;  51435 sec
[2024-04-16 07:44:19,191 INFO] Step 22150/50000; xent: 2.70; lr: 0.0000134;   9 docs/s;  51551 sec
[2024-04-16 07:46:14,380 INFO] Step 22200/50000; xent: 2.81; lr: 0.0000134;   9 docs/s;  51666 sec
[2024-04-16 07:48:11,214 INFO] Step 22250/50000; xent: 2.71; lr: 0.0000134;   9 docs/s;  51783 sec
[2024-04-16 07:48:41,043 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.32.bert.pt, number of examples: 1998
[2024-04-16 07:50:07,032 INFO] Step 22300/50000; xent: 2.71; lr: 0.0000134;   9 docs/s;  51898 sec
[2024-04-16 07:52:03,014 INFO] Step 22350/50000; xent: 2.74; lr: 0.0000134;   9 docs/s;  52014 sec
[2024-04-16 07:53:59,257 INFO] Step 22400/50000; xent: 2.70; lr: 0.0000134;   9 docs/s;  52131 sec
[2024-04-16 07:55:57,027 INFO] Step 22450/50000; xent: 2.80; lr: 0.0000133;   9 docs/s;  52248 sec
[2024-04-16 07:56:06,534 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.40.bert.pt, number of examples: 1999
[2024-04-16 07:57:54,281 INFO] Step 22500/50000; xent: 2.77; lr: 0.0000133;   9 docs/s;  52366 sec
[2024-04-16 07:59:49,855 INFO] Step 22550/50000; xent: 2.80; lr: 0.0000133;   9 docs/s;  52481 sec
[2024-04-16 08:01:44,179 INFO] Step 22600/50000; xent: 2.67; lr: 0.0000133;   9 docs/s;  52596 sec
[2024-04-16 08:03:25,438 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.43.bert.pt, number of examples: 2001
[2024-04-16 08:03:40,678 INFO] Step 22650/50000; xent: 2.74; lr: 0.0000133;   9 docs/s;  52712 sec
[2024-04-16 08:05:36,626 INFO] Step 22700/50000; xent: 2.76; lr: 0.0000133;   9 docs/s;  52828 sec
[2024-04-16 08:07:35,103 INFO] Step 22750/50000; xent: 2.71; lr: 0.0000133;   9 docs/s;  52947 sec
[2024-04-16 08:09:31,040 INFO] Step 22800/50000; xent: 2.77; lr: 0.0000132;   9 docs/s;  53062 sec
[2024-04-16 08:10:48,700 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.76.bert.pt, number of examples: 1999
[2024-04-16 08:11:25,960 INFO] Step 22850/50000; xent: 2.65; lr: 0.0000132;   9 docs/s;  53177 sec
[2024-04-16 08:13:22,188 INFO] Step 22900/50000; xent: 2.70; lr: 0.0000132;   9 docs/s;  53294 sec
[2024-04-16 08:15:19,096 INFO] Step 22950/50000; xent: 2.73; lr: 0.0000132;   9 docs/s;  53411 sec
[2024-04-16 08:17:15,182 INFO] Step 23000/50000; xent: 2.71; lr: 0.0000132;   9 docs/s;  53527 sec
[2024-04-16 08:17:15,193 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_23000.pt
[2024-04-16 08:18:13,874 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.129.bert.pt, number of examples: 2000
[2024-04-16 08:19:14,412 INFO] Step 23050/50000; xent: 2.81; lr: 0.0000132;   9 docs/s;  53646 sec
[2024-04-16 08:21:11,778 INFO] Step 23100/50000; xent: 2.73; lr: 0.0000132;   9 docs/s;  53763 sec
[2024-04-16 08:23:07,715 INFO] Step 23150/50000; xent: 2.75; lr: 0.0000131;   9 docs/s;  53879 sec
[2024-04-16 08:25:04,684 INFO] Step 23200/50000; xent: 2.69; lr: 0.0000131;   9 docs/s;  53996 sec
[2024-04-16 08:25:39,234 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.117.bert.pt, number of examples: 2000
[2024-04-16 08:27:00,417 INFO] Step 23250/50000; xent: 2.75; lr: 0.0000131;   9 docs/s;  54112 sec
[2024-04-16 08:28:55,387 INFO] Step 23300/50000; xent: 2.71; lr: 0.0000131;   9 docs/s;  54227 sec
[2024-04-16 08:30:55,291 INFO] Step 23350/50000; xent: 2.78; lr: 0.0000131;   9 docs/s;  54347 sec
[2024-04-16 08:32:51,075 INFO] Step 23400/50000; xent: 2.79; lr: 0.0000131;   9 docs/s;  54463 sec
[2024-04-16 08:33:00,630 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.115.bert.pt, number of examples: 2000
[2024-04-16 08:34:46,963 INFO] Step 23450/50000; xent: 2.71; lr: 0.0000131;   9 docs/s;  54578 sec
[2024-04-16 08:36:44,715 INFO] Step 23500/50000; xent: 2.68; lr: 0.0000130;   9 docs/s;  54696 sec
[2024-04-16 08:38:40,069 INFO] Step 23550/50000; xent: 2.72; lr: 0.0000130;   9 docs/s;  54812 sec
[2024-04-16 08:40:21,805 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.49.bert.pt, number of examples: 2001
[2024-04-16 08:40:36,049 INFO] Step 23600/50000; xent: 2.78; lr: 0.0000130;   9 docs/s;  54927 sec
[2024-04-16 08:42:32,152 INFO] Step 23650/50000; xent: 2.73; lr: 0.0000130;   9 docs/s;  55044 sec
[2024-04-16 08:44:29,169 INFO] Step 23700/50000; xent: 2.75; lr: 0.0000130;   9 docs/s;  55161 sec
[2024-04-16 08:46:24,761 INFO] Step 23750/50000; xent: 2.71; lr: 0.0000130;   9 docs/s;  55276 sec
[2024-04-16 08:47:43,604 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.132.bert.pt, number of examples: 2001
[2024-04-16 08:48:21,015 INFO] Step 23800/50000; xent: 2.73; lr: 0.0000130;   9 docs/s;  55392 sec
[2024-04-16 08:50:16,877 INFO] Step 23850/50000; xent: 2.78; lr: 0.0000130;   9 docs/s;  55508 sec
[2024-04-16 08:52:13,182 INFO] Step 23900/50000; xent: 2.76; lr: 0.0000129;   9 docs/s;  55625 sec
[2024-04-16 08:54:09,719 INFO] Step 23950/50000; xent: 2.81; lr: 0.0000129;   9 docs/s;  55741 sec
[2024-04-16 08:55:08,031 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.91.bert.pt, number of examples: 1998
[2024-04-16 08:56:06,308 INFO] Step 24000/50000; xent: 2.75; lr: 0.0000129;   9 docs/s;  55858 sec
[2024-04-16 08:56:06,317 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_24000.pt
[2024-04-16 08:58:04,720 INFO] Step 24050/50000; xent: 2.80; lr: 0.0000129;   9 docs/s;  55976 sec
[2024-04-16 09:00:01,198 INFO] Step 24100/50000; xent: 2.69; lr: 0.0000129;   9 docs/s;  56093 sec
[2024-04-16 09:01:58,335 INFO] Step 24150/50000; xent: 2.66; lr: 0.0000129;   9 docs/s;  56210 sec
[2024-04-16 09:02:31,053 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.31.bert.pt, number of examples: 2000
[2024-04-16 09:03:55,673 INFO] Step 24200/50000; xent: 2.73; lr: 0.0000129;   9 docs/s;  56327 sec
[2024-04-16 09:05:52,329 INFO] Step 24250/50000; xent: 2.81; lr: 0.0000128;   9 docs/s;  56444 sec
[2024-04-16 09:07:48,141 INFO] Step 24300/50000; xent: 2.63; lr: 0.0000128;   9 docs/s;  56560 sec
[2024-04-16 09:09:45,153 INFO] Step 24350/50000; xent: 2.70; lr: 0.0000128;   9 docs/s;  56677 sec
[2024-04-16 09:09:54,534 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.124.bert.pt, number of examples: 2001
[2024-04-16 09:11:41,161 INFO] Step 24400/50000; xent: 2.78; lr: 0.0000128;   9 docs/s;  56793 sec
[2024-04-16 09:13:39,606 INFO] Step 24450/50000; xent: 2.68; lr: 0.0000128;   9 docs/s;  56911 sec
[2024-04-16 09:15:35,931 INFO] Step 24500/50000; xent: 2.75; lr: 0.0000128;   9 docs/s;  57027 sec
[2024-04-16 09:17:21,042 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.101.bert.pt, number of examples: 2000
[2024-04-16 09:17:32,975 INFO] Step 24550/50000; xent: 2.80; lr: 0.0000128;   9 docs/s;  57144 sec
[2024-04-16 09:19:28,505 INFO] Step 24600/50000; xent: 2.71; lr: 0.0000128;   9 docs/s;  57260 sec
[2024-04-16 09:21:26,257 INFO] Step 24650/50000; xent: 2.72; lr: 0.0000127;   9 docs/s;  57378 sec
[2024-04-16 09:23:21,453 INFO] Step 24700/50000; xent: 2.68; lr: 0.0000127;   9 docs/s;  57493 sec
[2024-04-16 09:24:44,754 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.27.bert.pt, number of examples: 2001
[2024-04-16 09:25:17,380 INFO] Step 24750/50000; xent: 2.68; lr: 0.0000127;   9 docs/s;  57609 sec
[2024-04-16 09:27:16,439 INFO] Step 24800/50000; xent: 2.80; lr: 0.0000127;   9 docs/s;  57728 sec
[2024-04-16 09:29:12,315 INFO] Step 24850/50000; xent: 2.69; lr: 0.0000127;   9 docs/s;  57844 sec
[2024-04-16 09:31:08,308 INFO] Step 24900/50000; xent: 2.75; lr: 0.0000127;   9 docs/s;  57960 sec
[2024-04-16 09:32:09,005 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.78.bert.pt, number of examples: 2001
[2024-04-16 09:33:04,681 INFO] Step 24950/50000; xent: 2.81; lr: 0.0000127;   9 docs/s;  58076 sec
[2024-04-16 09:35:00,895 INFO] Step 25000/50000; xent: 2.74; lr: 0.0000126;   9 docs/s;  58192 sec
[2024-04-16 09:35:00,906 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_25000.pt
[2024-04-16 09:37:01,943 INFO] Step 25050/50000; xent: 2.66; lr: 0.0000126;   9 docs/s;  58313 sec
[2024-04-16 09:38:56,665 INFO] Step 25100/50000; xent: 2.73; lr: 0.0000126;   9 docs/s;  58428 sec
[2024-04-16 09:39:33,756 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.59.bert.pt, number of examples: 2000
[2024-04-16 09:40:51,776 INFO] Step 25150/50000; xent: 2.74; lr: 0.0000126;   9 docs/s;  58543 sec
[2024-04-16 09:42:49,160 INFO] Step 25200/50000; xent: 2.73; lr: 0.0000126;   9 docs/s;  58661 sec
[2024-04-16 09:44:46,891 INFO] Step 25250/50000; xent: 2.74; lr: 0.0000126;   9 docs/s;  58778 sec
[2024-04-16 09:46:42,261 INFO] Step 25300/50000; xent: 2.73; lr: 0.0000126;   9 docs/s;  58894 sec
[2024-04-16 09:46:56,311 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.130.bert.pt, number of examples: 2000
[2024-04-16 09:48:39,273 INFO] Step 25350/50000; xent: 2.80; lr: 0.0000126;   9 docs/s;  59011 sec
[2024-04-16 09:50:35,080 INFO] Step 25400/50000; xent: 2.74; lr: 0.0000125;   9 docs/s;  59127 sec
[2024-04-16 09:52:31,055 INFO] Step 25450/50000; xent: 2.81; lr: 0.0000125;   9 docs/s;  59242 sec
[2024-04-16 09:54:20,278 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.109.bert.pt, number of examples: 2000
[2024-04-16 09:54:29,531 INFO] Step 25500/50000; xent: 2.70; lr: 0.0000125;   9 docs/s;  59361 sec
[2024-04-16 09:56:26,389 INFO] Step 25550/50000; xent: 2.76; lr: 0.0000125;   9 docs/s;  59478 sec
[2024-04-16 09:58:23,985 INFO] Step 25600/50000; xent: 2.64; lr: 0.0000125;   9 docs/s;  59595 sec
[2024-04-16 10:00:20,287 INFO] Step 25650/50000; xent: 2.74; lr: 0.0000125;   9 docs/s;  59712 sec
[2024-04-16 10:01:43,797 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.126.bert.pt, number of examples: 1999
[2024-04-16 10:02:16,985 INFO] Step 25700/50000; xent: 2.75; lr: 0.0000125;   9 docs/s;  59828 sec
[2024-04-16 10:04:15,700 INFO] Step 25750/50000; xent: 2.77; lr: 0.0000125;   9 docs/s;  59947 sec
[2024-04-16 10:06:10,872 INFO] Step 25800/50000; xent: 2.69; lr: 0.0000125;   9 docs/s;  60062 sec
[2024-04-16 10:08:07,434 INFO] Step 25850/50000; xent: 2.75; lr: 0.0000124;   9 docs/s;  60179 sec
[2024-04-16 10:09:04,391 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.47.bert.pt, number of examples: 2000
[2024-04-16 10:10:02,389 INFO] Step 25900/50000; xent: 2.76; lr: 0.0000124;   9 docs/s;  60294 sec
[2024-04-16 10:11:59,017 INFO] Step 25950/50000; xent: 2.70; lr: 0.0000124;   9 docs/s;  60410 sec
[2024-04-16 10:13:55,851 INFO] Step 26000/50000; xent: 2.66; lr: 0.0000124;   9 docs/s;  60527 sec
[2024-04-16 10:13:55,861 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_26000.pt
[2024-04-16 10:15:55,364 INFO] Step 26050/50000; xent: 2.76; lr: 0.0000124;   9 docs/s;  60647 sec
[2024-04-16 10:16:30,258 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.100.bert.pt, number of examples: 1999
[2024-04-16 10:17:54,103 INFO] Step 26100/50000; xent: 2.68; lr: 0.0000124;   9 docs/s;  60766 sec
[2024-04-16 10:19:50,538 INFO] Step 26150/50000; xent: 2.70; lr: 0.0000124;   9 docs/s;  60882 sec
[2024-04-16 10:21:45,947 INFO] Step 26200/50000; xent: 2.73; lr: 0.0000124;   9 docs/s;  60997 sec
[2024-04-16 10:23:40,846 INFO] Step 26250/50000; xent: 2.80; lr: 0.0000123;   9 docs/s;  61112 sec
[2024-04-16 10:23:52,858 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.86.bert.pt, number of examples: 1999
[2024-04-16 10:25:37,945 INFO] Step 26300/50000; xent: 2.71; lr: 0.0000123;   9 docs/s;  61229 sec
[2024-04-16 10:27:33,994 INFO] Step 26350/50000; xent: 2.69; lr: 0.0000123;   9 docs/s;  61345 sec
[2024-04-16 10:29:32,760 INFO] Step 26400/50000; xent: 2.67; lr: 0.0000123;   9 docs/s;  61464 sec
[2024-04-16 10:31:18,530 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.35.bert.pt, number of examples: 1998
[2024-04-16 10:31:30,381 INFO] Step 26450/50000; xent: 2.67; lr: 0.0000123;   9 docs/s;  61582 sec
[2024-04-16 10:33:27,735 INFO] Step 26500/50000; xent: 2.72; lr: 0.0000123;   9 docs/s;  61699 sec
[2024-04-16 10:35:26,604 INFO] Step 26550/50000; xent: 2.72; lr: 0.0000123;   9 docs/s;  61818 sec
[2024-04-16 10:37:22,909 INFO] Step 26600/50000; xent: 2.74; lr: 0.0000123;   9 docs/s;  61934 sec
[2024-04-16 10:38:46,812 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.70.bert.pt, number of examples: 2000
[2024-04-16 10:39:19,480 INFO] Step 26650/50000; xent: 2.72; lr: 0.0000123;   9 docs/s;  62051 sec
[2024-04-16 10:41:15,294 INFO] Step 26700/50000; xent: 2.73; lr: 0.0000122;   9 docs/s;  62167 sec
[2024-04-16 10:43:12,189 INFO] Step 26750/50000; xent: 2.70; lr: 0.0000122;   9 docs/s;  62284 sec
[2024-04-16 10:45:10,226 INFO] Step 26800/50000; xent: 2.68; lr: 0.0000122;   9 docs/s;  62402 sec
[2024-04-16 10:46:14,185 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.57.bert.pt, number of examples: 1999
[2024-04-16 10:47:08,476 INFO] Step 26850/50000; xent: 2.74; lr: 0.0000122;   9 docs/s;  62520 sec
[2024-04-16 10:49:06,690 INFO] Step 26900/50000; xent: 2.75; lr: 0.0000122;   9 docs/s;  62638 sec
[2024-04-16 10:51:02,958 INFO] Step 26950/50000; xent: 2.72; lr: 0.0000122;   9 docs/s;  62754 sec
[2024-04-16 10:52:59,137 INFO] Step 27000/50000; xent: 2.72; lr: 0.0000122;   9 docs/s;  62871 sec
[2024-04-16 10:52:59,138 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_27000.pt
[2024-04-16 10:53:42,133 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.75.bert.pt, number of examples: 1999
[2024-04-16 10:54:59,148 INFO] Step 27050/50000; xent: 2.65; lr: 0.0000122;   9 docs/s;  62991 sec
[2024-04-16 10:56:55,028 INFO] Step 27100/50000; xent: 2.74; lr: 0.0000121;   9 docs/s;  63106 sec
[2024-04-16 10:58:52,966 INFO] Step 27150/50000; xent: 2.69; lr: 0.0000121;   9 docs/s;  63224 sec
[2024-04-16 11:00:50,058 INFO] Step 27200/50000; xent: 2.70; lr: 0.0000121;   9 docs/s;  63342 sec
[2024-04-16 11:01:09,052 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.135.bert.pt, number of examples: 1999
[2024-04-16 11:02:45,517 INFO] Step 27250/50000; xent: 2.75; lr: 0.0000121;   9 docs/s;  63457 sec
[2024-04-16 11:04:44,196 INFO] Step 27300/50000; xent: 2.67; lr: 0.0000121;   9 docs/s;  63576 sec
[2024-04-16 11:06:41,316 INFO] Step 27350/50000; xent: 2.70; lr: 0.0000121;   9 docs/s;  63693 sec
[2024-04-16 11:08:33,058 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.21.bert.pt, number of examples: 2001
[2024-04-16 11:08:37,693 INFO] Step 27400/50000; xent: 2.79; lr: 0.0000121;   9 docs/s;  63809 sec
[2024-04-16 11:10:33,938 INFO] Step 27450/50000; xent: 2.71; lr: 0.0000121;   9 docs/s;  63925 sec
[2024-04-16 11:12:30,441 INFO] Step 27500/50000; xent: 2.78; lr: 0.0000121;   9 docs/s;  64042 sec
[2024-04-16 11:14:29,381 INFO] Step 27550/50000; xent: 2.70; lr: 0.0000120;   9 docs/s;  64161 sec
[2024-04-16 11:15:57,850 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.99.bert.pt, number of examples: 2000
[2024-04-16 11:16:25,822 INFO] Step 27600/50000; xent: 2.68; lr: 0.0000120;   9 docs/s;  64277 sec
[2024-04-16 11:18:24,211 INFO] Step 27650/50000; xent: 2.65; lr: 0.0000120;   9 docs/s;  64396 sec
[2024-04-16 11:20:20,492 INFO] Step 27700/50000; xent: 2.67; lr: 0.0000120;   9 docs/s;  64512 sec
[2024-04-16 11:22:18,112 INFO] Step 27750/50000; xent: 2.66; lr: 0.0000120;   9 docs/s;  64630 sec
[2024-04-16 11:23:19,946 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.11.bert.pt, number of examples: 1999
[2024-04-16 11:24:14,231 INFO] Step 27800/50000; xent: 2.70; lr: 0.0000120;   9 docs/s;  64746 sec
[2024-04-16 11:26:10,580 INFO] Step 27850/50000; xent: 2.73; lr: 0.0000120;   9 docs/s;  64862 sec
[2024-04-16 11:28:08,849 INFO] Step 27900/50000; xent: 2.65; lr: 0.0000120;   9 docs/s;  64980 sec
[2024-04-16 11:30:05,357 INFO] Step 27950/50000; xent: 2.69; lr: 0.0000120;   9 docs/s;  65097 sec
[2024-04-16 11:30:45,145 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.67.bert.pt, number of examples: 1999
[2024-04-16 11:32:01,919 INFO] Step 28000/50000; xent: 2.65; lr: 0.0000120;   9 docs/s;  65213 sec
[2024-04-16 11:32:01,928 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_28000.pt
[2024-04-16 11:34:00,383 INFO] Step 28050/50000; xent: 2.64; lr: 0.0000119;   9 docs/s;  65332 sec
[2024-04-16 11:35:56,769 INFO] Step 28100/50000; xent: 2.59; lr: 0.0000119;   9 docs/s;  65448 sec
[2024-04-16 11:37:53,829 INFO] Step 28150/50000; xent: 2.58; lr: 0.0000119;   9 docs/s;  65565 sec
[2024-04-16 11:38:10,499 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.66.bert.pt, number of examples: 2001
[2024-04-16 11:39:50,242 INFO] Step 28200/50000; xent: 2.66; lr: 0.0000119;   9 docs/s;  65682 sec
[2024-04-16 11:41:46,542 INFO] Step 28250/50000; xent: 2.61; lr: 0.0000119;   9 docs/s;  65798 sec
[2024-04-16 11:43:43,537 INFO] Step 28300/50000; xent: 2.60; lr: 0.0000119;   9 docs/s;  65915 sec
[2024-04-16 11:45:35,820 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.45.bert.pt, number of examples: 2001
[2024-04-16 11:45:42,618 INFO] Step 28350/50000; xent: 2.59; lr: 0.0000119;   9 docs/s;  66034 sec
[2024-04-16 11:47:38,394 INFO] Step 28400/50000; xent: 2.66; lr: 0.0000119;   9 docs/s;  66150 sec
[2024-04-16 11:49:35,640 INFO] Step 28450/50000; xent: 2.61; lr: 0.0000119;   9 docs/s;  66267 sec
[2024-04-16 11:51:32,094 INFO] Step 28500/50000; xent: 2.62; lr: 0.0000118;   9 docs/s;  66384 sec
[2024-04-16 11:53:03,108 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.52.bert.pt, number of examples: 2001
[2024-04-16 11:53:28,791 INFO] Step 28550/50000; xent: 2.65; lr: 0.0000118;   9 docs/s;  66500 sec
[2024-04-16 11:55:25,183 INFO] Step 28600/50000; xent: 2.63; lr: 0.0000118;   9 docs/s;  66617 sec
[2024-04-16 11:57:24,419 INFO] Step 28650/50000; xent: 2.71; lr: 0.0000118;   9 docs/s;  66736 sec
[2024-04-16 11:59:20,552 INFO] Step 28700/50000; xent: 2.64; lr: 0.0000118;   9 docs/s;  66852 sec
[2024-04-16 12:00:26,944 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.36.bert.pt, number of examples: 2000
[2024-04-16 12:01:15,742 INFO] Step 28750/50000; xent: 2.74; lr: 0.0000118;   9 docs/s;  66967 sec
[2024-04-16 12:03:12,028 INFO] Step 28800/50000; xent: 2.54; lr: 0.0000118;   9 docs/s;  67083 sec
[2024-04-16 12:05:08,364 INFO] Step 28850/50000; xent: 2.61; lr: 0.0000118;   9 docs/s;  67200 sec
[2024-04-16 12:07:05,078 INFO] Step 28900/50000; xent: 2.65; lr: 0.0000118;   9 docs/s;  67317 sec
[2024-04-16 12:07:50,698 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.43.bert.pt, number of examples: 2001
[2024-04-16 12:09:02,399 INFO] Step 28950/50000; xent: 2.62; lr: 0.0000118;   9 docs/s;  67434 sec
[2024-04-16 12:11:00,397 INFO] Step 29000/50000; xent: 2.63; lr: 0.0000117;   9 docs/s;  67552 sec
[2024-04-16 12:11:00,406 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_29000.pt
[2024-04-16 12:12:59,614 INFO] Step 29050/50000; xent: 2.60; lr: 0.0000117;   9 docs/s;  67671 sec
[2024-04-16 12:14:56,260 INFO] Step 29100/50000; xent: 2.60; lr: 0.0000117;   9 docs/s;  67788 sec
[2024-04-16 12:15:21,287 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.13.bert.pt, number of examples: 2001
[2024-04-16 12:16:51,701 INFO] Step 29150/50000; xent: 2.74; lr: 0.0000117;   9 docs/s;  67903 sec
[2024-04-16 12:18:49,078 INFO] Step 29200/50000; xent: 2.71; lr: 0.0000117;   9 docs/s;  68021 sec
[2024-04-16 12:20:45,879 INFO] Step 29250/50000; xent: 2.73; lr: 0.0000117;   9 docs/s;  68137 sec
[2024-04-16 12:22:44,652 INFO] Step 29300/50000; xent: 2.77; lr: 0.0000117;   9 docs/s;  68256 sec
[2024-04-16 12:22:49,589 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.47.bert.pt, number of examples: 2000
[2024-04-16 12:24:42,465 INFO] Step 29350/50000; xent: 2.61; lr: 0.0000117;   9 docs/s;  68374 sec
[2024-04-16 12:26:40,138 INFO] Step 29400/50000; xent: 2.61; lr: 0.0000117;   9 docs/s;  68492 sec
[2024-04-16 12:28:36,033 INFO] Step 29450/50000; xent: 2.58; lr: 0.0000117;   9 docs/s;  68607 sec
[2024-04-16 12:30:14,819 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.48.bert.pt, number of examples: 2000
[2024-04-16 12:30:33,308 INFO] Step 29500/50000; xent: 2.60; lr: 0.0000116;   9 docs/s;  68725 sec
[2024-04-16 12:32:33,351 INFO] Step 29550/50000; xent: 2.60; lr: 0.0000116;   9 docs/s;  68845 sec
[2024-04-16 12:34:29,769 INFO] Step 29600/50000; xent: 2.67; lr: 0.0000116;   9 docs/s;  68961 sec
[2024-04-16 12:36:26,594 INFO] Step 29650/50000; xent: 2.62; lr: 0.0000116;   9 docs/s;  69078 sec
[2024-04-16 12:37:42,946 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.73.bert.pt, number of examples: 2000
[2024-04-16 12:38:22,902 INFO] Step 29700/50000; xent: 2.61; lr: 0.0000116;   9 docs/s;  69194 sec
[2024-04-16 12:40:19,610 INFO] Step 29750/50000; xent: 2.73; lr: 0.0000116;   9 docs/s;  69311 sec
[2024-04-16 12:42:17,774 INFO] Step 29800/50000; xent: 2.65; lr: 0.0000116;   9 docs/s;  69429 sec
[2024-04-16 12:44:16,127 INFO] Step 29850/50000; xent: 2.71; lr: 0.0000116;   9 docs/s;  69548 sec
[2024-04-16 12:45:08,855 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.62.bert.pt, number of examples: 2001
[2024-04-16 12:46:11,915 INFO] Step 29900/50000; xent: 2.64; lr: 0.0000116;   9 docs/s;  69663 sec
[2024-04-16 12:48:08,981 INFO] Step 29950/50000; xent: 2.68; lr: 0.0000116;   9 docs/s;  69780 sec
[2024-04-16 12:50:06,425 INFO] Step 30000/50000; xent: 2.71; lr: 0.0000115;   9 docs/s;  69898 sec
[2024-04-16 12:50:06,433 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_30000.pt
[2024-04-16 12:52:05,633 INFO] Step 30050/50000; xent: 2.76; lr: 0.0000115;   9 docs/s;  70017 sec
[2024-04-16 12:52:35,963 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.90.bert.pt, number of examples: 2000
[2024-04-16 12:54:02,362 INFO] Step 30100/50000; xent: 2.61; lr: 0.0000115;   9 docs/s;  70134 sec
[2024-04-16 12:55:58,485 INFO] Step 30150/50000; xent: 2.64; lr: 0.0000115;   9 docs/s;  70250 sec
[2024-04-16 12:57:56,593 INFO] Step 30200/50000; xent: 2.60; lr: 0.0000115;   9 docs/s;  70368 sec
[2024-04-16 12:59:56,254 INFO] Step 30250/50000; xent: 2.55; lr: 0.0000115;   9 docs/s;  70488 sec
[2024-04-16 13:00:05,950 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.97.bert.pt, number of examples: 2001
[2024-04-16 13:01:53,156 INFO] Step 30300/50000; xent: 2.78; lr: 0.0000115;   9 docs/s;  70605 sec
[2024-04-16 13:03:52,080 INFO] Step 30350/50000; xent: 2.66; lr: 0.0000115;   9 docs/s;  70724 sec
[2024-04-16 13:05:48,032 INFO] Step 30400/50000; xent: 2.68; lr: 0.0000115;   9 docs/s;  70839 sec
[2024-04-16 13:07:27,971 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.14.bert.pt, number of examples: 1998
[2024-04-16 13:07:44,180 INFO] Step 30450/50000; xent: 2.59; lr: 0.0000115;   9 docs/s;  70956 sec
[2024-04-16 13:09:39,070 INFO] Step 30500/50000; xent: 2.64; lr: 0.0000115;   9 docs/s;  71071 sec
[2024-04-16 13:11:35,938 INFO] Step 30550/50000; xent: 2.60; lr: 0.0000114;   9 docs/s;  71187 sec
[2024-04-16 13:13:35,932 INFO] Step 30600/50000; xent: 2.71; lr: 0.0000114;   9 docs/s;  71307 sec
[2024-04-16 13:14:55,739 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.28.bert.pt, number of examples: 2000
[2024-04-16 13:15:32,600 INFO] Step 30650/50000; xent: 2.60; lr: 0.0000114;   9 docs/s;  71424 sec
[2024-04-16 13:17:28,463 INFO] Step 30700/50000; xent: 2.62; lr: 0.0000114;   9 docs/s;  71540 sec
[2024-04-16 13:19:26,206 INFO] Step 30750/50000; xent: 2.62; lr: 0.0000114;   9 docs/s;  71658 sec
[2024-04-16 13:21:24,840 INFO] Step 30800/50000; xent: 2.68; lr: 0.0000114;   9 docs/s;  71776 sec
[2024-04-16 13:22:20,921 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.94.bert.pt, number of examples: 2001
[2024-04-16 13:23:21,811 INFO] Step 30850/50000; xent: 2.69; lr: 0.0000114;   9 docs/s;  71893 sec
[2024-04-16 13:25:18,080 INFO] Step 30900/50000; xent: 2.63; lr: 0.0000114;   9 docs/s;  72010 sec
[2024-04-16 13:27:16,066 INFO] Step 30950/50000; xent: 2.72; lr: 0.0000114;   9 docs/s;  72128 sec
[2024-04-16 13:29:16,120 INFO] Step 31000/50000; xent: 2.74; lr: 0.0000114;   9 docs/s;  72248 sec
[2024-04-16 13:29:16,122 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_31000.pt
[2024-04-16 13:29:53,635 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.4.bert.pt, number of examples: 2001
[2024-04-16 13:31:16,090 INFO] Step 31050/50000; xent: 2.65; lr: 0.0000114;   9 docs/s;  72368 sec
[2024-04-16 13:33:12,861 INFO] Step 31100/50000; xent: 2.77; lr: 0.0000113;   9 docs/s;  72484 sec
[2024-04-16 13:35:09,292 INFO] Step 31150/50000; xent: 2.62; lr: 0.0000113;   9 docs/s;  72601 sec
[2024-04-16 13:37:04,872 INFO] Step 31200/50000; xent: 2.60; lr: 0.0000113;   9 docs/s;  72716 sec
[2024-04-16 13:37:19,457 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.39.bert.pt, number of examples: 2000
[2024-04-16 13:39:00,585 INFO] Step 31250/50000; xent: 2.65; lr: 0.0000113;   9 docs/s;  72832 sec
[2024-04-16 13:40:57,483 INFO] Step 31300/50000; xent: 2.69; lr: 0.0000113;   9 docs/s;  72949 sec
[2024-04-16 13:42:54,374 INFO] Step 31350/50000; xent: 2.71; lr: 0.0000113;   9 docs/s;  73066 sec
[2024-04-16 13:44:42,478 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.119.bert.pt, number of examples: 2000
[2024-04-16 13:44:51,787 INFO] Step 31400/50000; xent: 2.70; lr: 0.0000113;   9 docs/s;  73183 sec
[2024-04-16 13:46:49,189 INFO] Step 31450/50000; xent: 2.62; lr: 0.0000113;   9 docs/s;  73301 sec
[2024-04-16 13:48:49,656 INFO] Step 31500/50000; xent: 2.64; lr: 0.0000113;   9 docs/s;  73421 sec
[2024-04-16 13:50:46,620 INFO] Step 31550/50000; xent: 2.65; lr: 0.0000113;   9 docs/s;  73538 sec
[2024-04-16 13:52:13,408 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.38.bert.pt, number of examples: 2001
[2024-04-16 13:52:43,415 INFO] Step 31600/50000; xent: 2.64; lr: 0.0000113;   9 docs/s;  73655 sec
[2024-04-16 13:54:40,835 INFO] Step 31650/50000; xent: 2.74; lr: 0.0000112;   9 docs/s;  73772 sec
[2024-04-16 13:56:37,040 INFO] Step 31700/50000; xent: 2.57; lr: 0.0000112;   9 docs/s;  73888 sec
[2024-04-16 13:58:33,291 INFO] Step 31750/50000; xent: 2.73; lr: 0.0000112;   9 docs/s;  74005 sec
[2024-04-16 13:59:37,585 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.2.bert.pt, number of examples: 2001
[2024-04-16 14:00:29,087 INFO] Step 31800/50000; xent: 2.60; lr: 0.0000112;   9 docs/s;  74121 sec
[2024-04-16 14:02:24,617 INFO] Step 31850/50000; xent: 2.62; lr: 0.0000112;   9 docs/s;  74236 sec
[2024-04-16 14:04:21,951 INFO] Step 31900/50000; xent: 2.57; lr: 0.0000112;   9 docs/s;  74353 sec
[2024-04-16 14:06:21,530 INFO] Step 31950/50000; xent: 2.61; lr: 0.0000112;   9 docs/s;  74473 sec
[2024-04-16 14:07:03,416 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.78.bert.pt, number of examples: 2001
[2024-04-16 14:08:17,678 INFO] Step 32000/50000; xent: 2.67; lr: 0.0000112;   9 docs/s;  74589 sec
[2024-04-16 14:08:17,687 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_32000.pt
[2024-04-16 14:10:16,964 INFO] Step 32050/50000; xent: 2.66; lr: 0.0000112;   9 docs/s;  74708 sec
[2024-04-16 14:12:13,581 INFO] Step 32100/50000; xent: 2.60; lr: 0.0000112;   9 docs/s;  74825 sec
[2024-04-16 14:14:09,347 INFO] Step 32150/50000; xent: 2.58; lr: 0.0000112;   9 docs/s;  74941 sec
[2024-04-16 14:14:30,308 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.55.bert.pt, number of examples: 1999
[2024-04-16 14:16:07,999 INFO] Step 32200/50000; xent: 2.55; lr: 0.0000111;   9 docs/s;  75059 sec
[2024-04-16 14:18:07,512 INFO] Step 32250/50000; xent: 2.64; lr: 0.0000111;   9 docs/s;  75179 sec
[2024-04-16 14:20:03,891 INFO] Step 32300/50000; xent: 2.58; lr: 0.0000111;   9 docs/s;  75295 sec
[2024-04-16 14:21:52,604 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.129.bert.pt, number of examples: 2000
[2024-04-16 14:21:59,870 INFO] Step 32350/50000; xent: 2.59; lr: 0.0000111;   9 docs/s;  75411 sec
[2024-04-16 14:23:56,625 INFO] Step 32400/50000; xent: 2.65; lr: 0.0000111;   9 docs/s;  75528 sec
[2024-04-16 14:25:52,138 INFO] Step 32450/50000; xent: 2.64; lr: 0.0000111;   9 docs/s;  75644 sec
[2024-04-16 14:27:50,488 INFO] Step 32500/50000; xent: 2.61; lr: 0.0000111;   9 docs/s;  75762 sec
[2024-04-16 14:29:21,184 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.30.bert.pt, number of examples: 1996
[2024-04-16 14:29:48,926 INFO] Step 32550/50000; xent: 2.61; lr: 0.0000111;   9 docs/s;  75880 sec
[2024-04-16 14:31:48,021 INFO] Step 32600/50000; xent: 2.61; lr: 0.0000111;   9 docs/s;  75999 sec
[2024-04-16 14:33:46,451 INFO] Step 32650/50000; xent: 2.62; lr: 0.0000111;   9 docs/s;  76118 sec
[2024-04-16 14:35:42,112 INFO] Step 32700/50000; xent: 2.63; lr: 0.0000111;   9 docs/s;  76234 sec
[2024-04-16 14:36:47,291 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.96.bert.pt, number of examples: 2001
[2024-04-16 14:37:38,615 INFO] Step 32750/50000; xent: 2.65; lr: 0.0000111;   9 docs/s;  76350 sec
[2024-04-16 14:39:34,665 INFO] Step 32800/50000; xent: 2.64; lr: 0.0000110;   9 docs/s;  76466 sec
[2024-04-16 14:41:31,793 INFO] Step 32850/50000; xent: 2.66; lr: 0.0000110;   9 docs/s;  76583 sec
[2024-04-16 14:43:29,479 INFO] Step 32900/50000; xent: 2.64; lr: 0.0000110;   9 docs/s;  76701 sec
[2024-04-16 14:44:13,647 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.128.bert.pt, number of examples: 2001
[2024-04-16 14:45:25,911 INFO] Step 32950/50000; xent: 2.61; lr: 0.0000110;   9 docs/s;  76817 sec
[2024-04-16 14:47:24,650 INFO] Step 33000/50000; xent: 2.62; lr: 0.0000110;   9 docs/s;  76936 sec
[2024-04-16 14:47:24,659 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_33000.pt
[2024-04-16 14:49:24,049 INFO] Step 33050/50000; xent: 2.67; lr: 0.0000110;   9 docs/s;  77055 sec
[2024-04-16 14:51:22,605 INFO] Step 33100/50000; xent: 2.61; lr: 0.0000110;   9 docs/s;  77174 sec
[2024-04-16 14:51:43,395 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.109.bert.pt, number of examples: 2000
[2024-04-16 14:53:21,204 INFO] Step 33150/50000; xent: 2.65; lr: 0.0000110;   9 docs/s;  77293 sec
[2024-04-16 14:55:17,602 INFO] Step 33200/50000; xent: 2.57; lr: 0.0000110;   9 docs/s;  77409 sec
[2024-04-16 14:57:15,765 INFO] Step 33250/50000; xent: 2.63; lr: 0.0000110;   9 docs/s;  77527 sec
[2024-04-16 14:59:08,509 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.103.bert.pt, number of examples: 2000
[2024-04-16 14:59:13,167 INFO] Step 33300/50000; xent: 2.60; lr: 0.0000110;   9 docs/s;  77645 sec
[2024-04-16 15:01:09,238 INFO] Step 33350/50000; xent: 2.61; lr: 0.0000110;   9 docs/s;  77761 sec
[2024-04-16 15:03:05,275 INFO] Step 33400/50000; xent: 2.62; lr: 0.0000109;   9 docs/s;  77877 sec
[2024-04-16 15:05:04,817 INFO] Step 33450/50000; xent: 2.60; lr: 0.0000109;   9 docs/s;  77996 sec
[2024-04-16 15:06:35,613 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.113.bert.pt, number of examples: 1999
[2024-04-16 15:07:01,770 INFO] Step 33500/50000; xent: 2.59; lr: 0.0000109;   9 docs/s;  78113 sec
[2024-04-16 15:08:58,889 INFO] Step 33550/50000; xent: 2.63; lr: 0.0000109;   9 docs/s;  78230 sec
[2024-04-16 15:10:56,124 INFO] Step 33600/50000; xent: 2.62; lr: 0.0000109;   9 docs/s;  78348 sec
[2024-04-16 15:12:51,821 INFO] Step 33650/50000; xent: 2.70; lr: 0.0000109;   9 docs/s;  78463 sec
[2024-04-16 15:13:59,275 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.91.bert.pt, number of examples: 1998
[2024-04-16 15:14:48,392 INFO] Step 33700/50000; xent: 2.72; lr: 0.0000109;   9 docs/s;  78580 sec
[2024-04-16 15:16:45,821 INFO] Step 33750/50000; xent: 2.58; lr: 0.0000109;   9 docs/s;  78697 sec
[2024-04-16 15:18:43,169 INFO] Step 33800/50000; xent: 2.65; lr: 0.0000109;   9 docs/s;  78815 sec
[2024-04-16 15:20:41,325 INFO] Step 33850/50000; xent: 2.58; lr: 0.0000109;   9 docs/s;  78933 sec
[2024-04-16 15:21:23,649 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.64.bert.pt, number of examples: 2001
[2024-04-16 15:22:40,575 INFO] Step 33900/50000; xent: 2.68; lr: 0.0000109;   9 docs/s;  79052 sec
[2024-04-16 15:24:38,193 INFO] Step 33950/50000; xent: 2.66; lr: 0.0000109;   9 docs/s;  79170 sec
[2024-04-16 15:26:35,180 INFO] Step 34000/50000; xent: 2.70; lr: 0.0000108;   9 docs/s;  79287 sec
[2024-04-16 15:26:35,190 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_34000.pt
[2024-04-16 15:28:34,858 INFO] Step 34050/50000; xent: 2.70; lr: 0.0000108;   9 docs/s;  79406 sec
[2024-04-16 15:28:53,932 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.41.bert.pt, number of examples: 2000
[2024-04-16 15:30:32,252 INFO] Step 34100/50000; xent: 2.64; lr: 0.0000108;   9 docs/s;  79524 sec
[2024-04-16 15:32:29,511 INFO] Step 34150/50000; xent: 2.64; lr: 0.0000108;   9 docs/s;  79641 sec
[2024-04-16 15:34:26,827 INFO] Step 34200/50000; xent: 2.61; lr: 0.0000108;   9 docs/s;  79758 sec
[2024-04-16 15:36:18,906 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.107.bert.pt, number of examples: 1999
[2024-04-16 15:36:23,586 INFO] Step 34250/50000; xent: 2.60; lr: 0.0000108;   9 docs/s;  79875 sec
[2024-04-16 15:38:19,838 INFO] Step 34300/50000; xent: 2.56; lr: 0.0000108;   9 docs/s;  79991 sec
[2024-04-16 15:40:17,343 INFO] Step 34350/50000; xent: 2.70; lr: 0.0000108;   9 docs/s;  80109 sec
[2024-04-16 15:42:16,444 INFO] Step 34400/50000; xent: 2.67; lr: 0.0000108;   9 docs/s;  80228 sec
[2024-04-16 15:43:45,476 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.106.bert.pt, number of examples: 1999
[2024-04-16 15:44:13,835 INFO] Step 34450/50000; xent: 2.65; lr: 0.0000108;   9 docs/s;  80345 sec
[2024-04-16 15:46:10,607 INFO] Step 34500/50000; xent: 2.71; lr: 0.0000108;   9 docs/s;  80462 sec
[2024-04-16 15:48:07,780 INFO] Step 34550/50000; xent: 2.73; lr: 0.0000108;   9 docs/s;  80579 sec
[2024-04-16 15:50:05,754 INFO] Step 34600/50000; xent: 2.71; lr: 0.0000108;   9 docs/s;  80697 sec
[2024-04-16 15:51:11,005 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.59.bert.pt, number of examples: 2000
[2024-04-16 15:52:03,357 INFO] Step 34650/50000; xent: 2.73; lr: 0.0000107;   9 docs/s;  80815 sec
[2024-04-16 15:53:59,612 INFO] Step 34700/50000; xent: 2.64; lr: 0.0000107;   9 docs/s;  80931 sec
[2024-04-16 15:55:56,418 INFO] Step 34750/50000; xent: 2.59; lr: 0.0000107;   9 docs/s;  81048 sec
[2024-04-16 15:57:53,661 INFO] Step 34800/50000; xent: 2.59; lr: 0.0000107;   9 docs/s;  81165 sec
[2024-04-16 15:58:36,426 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.12.bert.pt, number of examples: 2001
[2024-04-16 15:59:53,592 INFO] Step 34850/50000; xent: 2.63; lr: 0.0000107;   9 docs/s;  81285 sec
[2024-04-16 16:01:50,723 INFO] Step 34900/50000; xent: 2.72; lr: 0.0000107;   9 docs/s;  81402 sec
[2024-04-16 16:03:47,256 INFO] Step 34950/50000; xent: 2.69; lr: 0.0000107;   9 docs/s;  81519 sec
[2024-04-16 16:05:44,384 INFO] Step 35000/50000; xent: 2.74; lr: 0.0000107;   9 docs/s;  81636 sec
[2024-04-16 16:05:44,385 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_35000.pt
[2024-04-16 16:06:07,918 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.26.bert.pt, number of examples: 2000
[2024-04-16 16:07:42,945 INFO] Step 35050/50000; xent: 2.63; lr: 0.0000107;   9 docs/s;  81754 sec
[2024-04-16 16:09:38,894 INFO] Step 35100/50000; xent: 2.60; lr: 0.0000107;   9 docs/s;  81870 sec
[2024-04-16 16:11:36,338 INFO] Step 35150/50000; xent: 2.58; lr: 0.0000107;   9 docs/s;  81988 sec
[2024-04-16 16:13:33,569 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.46.bert.pt, number of examples: 2001
[2024-04-16 16:13:35,891 INFO] Step 35200/50000; xent: 2.67; lr: 0.0000107;   9 docs/s;  82107 sec
[2024-04-16 16:15:32,645 INFO] Step 35250/50000; xent: 2.71; lr: 0.0000107;   9 docs/s;  82224 sec
[2024-04-16 16:17:30,247 INFO] Step 35300/50000; xent: 2.76; lr: 0.0000106;   9 docs/s;  82342 sec
[2024-04-16 16:19:26,762 INFO] Step 35350/50000; xent: 2.68; lr: 0.0000106;   9 docs/s;  82458 sec
[2024-04-16 16:20:59,404 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.65.bert.pt, number of examples: 2000
[2024-04-16 16:21:24,727 INFO] Step 35400/50000; xent: 2.70; lr: 0.0000106;   9 docs/s;  82576 sec
[2024-04-16 16:23:21,399 INFO] Step 35450/50000; xent: 2.59; lr: 0.0000106;   9 docs/s;  82693 sec
[2024-04-16 16:25:18,584 INFO] Step 35500/50000; xent: 2.68; lr: 0.0000106;   9 docs/s;  82810 sec
[2024-04-16 16:27:16,246 INFO] Step 35550/50000; xent: 2.64; lr: 0.0000106;   9 docs/s;  82928 sec
[2024-04-16 16:28:26,965 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.72.bert.pt, number of examples: 2000
[2024-04-16 16:29:15,517 INFO] Step 35600/50000; xent: 2.58; lr: 0.0000106;   9 docs/s;  83047 sec
[2024-04-16 16:31:15,103 INFO] Step 35650/50000; xent: 2.69; lr: 0.0000106;   9 docs/s;  83167 sec
[2024-04-16 16:33:22,767 INFO] Step 35700/50000; xent: 2.67; lr: 0.0000106;   8 docs/s;  83294 sec
[2024-04-16 16:35:22,601 INFO] Step 35750/50000; xent: 2.56; lr: 0.0000106;   9 docs/s;  83414 sec
[2024-04-16 16:36:09,899 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.102.bert.pt, number of examples: 1999
[2024-04-16 16:37:20,521 INFO] Step 35800/50000; xent: 2.67; lr: 0.0000106;   9 docs/s;  83532 sec
[2024-04-16 16:39:20,622 INFO] Step 35850/50000; xent: 2.62; lr: 0.0000106;   9 docs/s;  83652 sec
[2024-04-16 16:41:19,519 INFO] Step 35900/50000; xent: 2.76; lr: 0.0000106;   9 docs/s;  83771 sec
[2024-04-16 16:43:20,366 INFO] Step 35950/50000; xent: 2.72; lr: 0.0000105;   8 docs/s;  83892 sec
[2024-04-16 16:43:46,812 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.133.bert.pt, number of examples: 1999
[2024-04-16 16:45:19,435 INFO] Step 36000/50000; xent: 2.69; lr: 0.0000105;   9 docs/s;  84011 sec
[2024-04-16 16:45:19,448 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_36000.pt
[2024-04-16 16:47:22,108 INFO] Step 36050/50000; xent: 2.52; lr: 0.0000105;   8 docs/s;  84134 sec
[2024-04-16 16:49:20,435 INFO] Step 36100/50000; xent: 2.62; lr: 0.0000105;   9 docs/s;  84252 sec
[2024-04-16 16:51:17,289 INFO] Step 36150/50000; xent: 2.69; lr: 0.0000105;   9 docs/s;  84369 sec
[2024-04-16 16:51:19,924 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.114.bert.pt, number of examples: 1998
[2024-04-16 16:53:14,611 INFO] Step 36200/50000; xent: 2.73; lr: 0.0000105;   9 docs/s;  84486 sec
[2024-04-16 16:55:11,378 INFO] Step 36250/50000; xent: 2.64; lr: 0.0000105;   9 docs/s;  84603 sec
[2024-04-16 16:57:07,372 INFO] Step 36300/50000; xent: 2.70; lr: 0.0000105;   9 docs/s;  84719 sec
[2024-04-16 16:58:41,725 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.49.bert.pt, number of examples: 2001
[2024-04-16 16:59:04,918 INFO] Step 36350/50000; xent: 2.63; lr: 0.0000105;   9 docs/s;  84836 sec
[2024-04-16 17:01:02,029 INFO] Step 36400/50000; xent: 2.59; lr: 0.0000105;   9 docs/s;  84953 sec
[2024-04-16 17:02:58,288 INFO] Step 36450/50000; xent: 2.65; lr: 0.0000105;   9 docs/s;  85070 sec
[2024-04-16 17:04:55,534 INFO] Step 36500/50000; xent: 2.62; lr: 0.0000105;   9 docs/s;  85187 sec
[2024-04-16 17:06:05,842 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.95.bert.pt, number of examples: 2001
[2024-04-16 17:06:53,773 INFO] Step 36550/50000; xent: 2.62; lr: 0.0000105;   9 docs/s;  85305 sec
[2024-04-16 17:08:50,103 INFO] Step 36600/50000; xent: 2.69; lr: 0.0000105;   9 docs/s;  85422 sec
[2024-04-16 17:10:44,764 INFO] Step 36650/50000; xent: 2.68; lr: 0.0000104;   9 docs/s;  85536 sec
[2024-04-16 17:12:43,950 INFO] Step 36700/50000; xent: 2.65; lr: 0.0000104;   9 docs/s;  85655 sec
[2024-04-16 17:13:34,710 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.34.bert.pt, number of examples: 2000
[2024-04-16 17:14:38,990 INFO] Step 36750/50000; xent: 2.76; lr: 0.0000104;   9 docs/s;  85770 sec
[2024-04-16 17:16:35,541 INFO] Step 36800/50000; xent: 2.66; lr: 0.0000104;   9 docs/s;  85887 sec
[2024-04-16 17:18:33,595 INFO] Step 36850/50000; xent: 2.66; lr: 0.0000104;   9 docs/s;  86005 sec
[2024-04-16 17:20:30,954 INFO] Step 36900/50000; xent: 2.57; lr: 0.0000104;   9 docs/s;  86122 sec
[2024-04-16 17:20:58,425 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.42.bert.pt, number of examples: 2000
[2024-04-16 17:22:27,359 INFO] Step 36950/50000; xent: 2.65; lr: 0.0000104;   9 docs/s;  86239 sec
[2024-04-16 17:24:24,022 INFO] Step 37000/50000; xent: 2.74; lr: 0.0000104;   9 docs/s;  86355 sec
[2024-04-16 17:24:24,032 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_37000.pt
[2024-04-16 17:26:24,180 INFO] Step 37050/50000; xent: 2.74; lr: 0.0000104;   9 docs/s;  86476 sec
[2024-04-16 17:28:20,030 INFO] Step 37100/50000; xent: 2.75; lr: 0.0000104;   9 docs/s;  86591 sec
[2024-04-16 17:28:24,846 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.33.bert.pt, number of examples: 1999
[2024-04-16 17:30:16,838 INFO] Step 37150/50000; xent: 2.69; lr: 0.0000104;   9 docs/s;  86708 sec
[2024-04-16 17:32:13,409 INFO] Step 37200/50000; xent: 2.64; lr: 0.0000104;   9 docs/s;  86825 sec
[2024-04-16 17:34:13,596 INFO] Step 37250/50000; xent: 2.61; lr: 0.0000104;   9 docs/s;  86945 sec
[2024-04-16 17:35:50,782 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.83.bert.pt, number of examples: 1999
[2024-04-16 17:36:11,682 INFO] Step 37300/50000; xent: 2.66; lr: 0.0000104;   9 docs/s;  87063 sec
[2024-04-16 17:38:08,650 INFO] Step 37350/50000; xent: 2.73; lr: 0.0000103;   9 docs/s;  87180 sec
[2024-04-16 17:40:05,400 INFO] Step 37400/50000; xent: 2.65; lr: 0.0000103;   9 docs/s;  87297 sec
[2024-04-16 17:42:03,939 INFO] Step 37450/50000; xent: 2.68; lr: 0.0000103;   9 docs/s;  87415 sec
[2024-04-16 17:43:14,363 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.6.bert.pt, number of examples: 2001
[2024-04-16 17:44:00,347 INFO] Step 37500/50000; xent: 2.70; lr: 0.0000103;   9 docs/s;  87532 sec
[2024-04-16 17:45:58,905 INFO] Step 37550/50000; xent: 2.68; lr: 0.0000103;   9 docs/s;  87650 sec
[2024-04-16 17:47:56,647 INFO] Step 37600/50000; xent: 2.83; lr: 0.0000103;   9 docs/s;  87768 sec
[2024-04-16 17:49:53,270 INFO] Step 37650/50000; xent: 2.65; lr: 0.0000103;   9 docs/s;  87885 sec
[2024-04-16 17:50:43,926 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.137.bert.pt, number of examples: 2000
[2024-04-16 17:51:49,601 INFO] Step 37700/50000; xent: 2.73; lr: 0.0000103;   9 docs/s;  88001 sec
[2024-04-16 17:53:45,230 INFO] Step 37750/50000; xent: 2.74; lr: 0.0000103;   9 docs/s;  88117 sec
[2024-04-16 17:55:43,714 INFO] Step 37800/50000; xent: 2.77; lr: 0.0000103;   9 docs/s;  88235 sec
[2024-04-16 17:57:40,523 INFO] Step 37850/50000; xent: 2.70; lr: 0.0000103;   9 docs/s;  88352 sec
[2024-04-16 17:58:09,281 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.86.bert.pt, number of examples: 1999
[2024-04-16 17:59:36,697 INFO] Step 37900/50000; xent: 2.67; lr: 0.0000103;   9 docs/s;  88468 sec
[2024-04-16 18:01:35,310 INFO] Step 37950/50000; xent: 2.59; lr: 0.0000103;   9 docs/s;  88587 sec
[2024-04-16 18:03:33,325 INFO] Step 38000/50000; xent: 2.54; lr: 0.0000103;   9 docs/s;  88705 sec
[2024-04-16 18:03:33,335 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_38000.pt
[2024-04-16 18:05:32,885 INFO] Step 38050/50000; xent: 2.54; lr: 0.0000103;   9 docs/s;  88824 sec
[2024-04-16 18:05:37,788 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.120.bert.pt, number of examples: 2001
[2024-04-16 18:07:31,687 INFO] Step 38100/50000; xent: 2.66; lr: 0.0000102;   9 docs/s;  88943 sec
[2024-04-16 18:09:28,867 INFO] Step 38150/50000; xent: 2.73; lr: 0.0000102;   9 docs/s;  89060 sec
[2024-04-16 18:11:26,324 INFO] Step 38200/50000; xent: 2.77; lr: 0.0000102;   9 docs/s;  89178 sec
[2024-04-16 18:13:03,941 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.85.bert.pt, number of examples: 2001
[2024-04-16 18:13:26,212 INFO] Step 38250/50000; xent: 2.73; lr: 0.0000102;   9 docs/s;  89298 sec
[2024-04-16 18:15:24,109 INFO] Step 38300/50000; xent: 2.67; lr: 0.0000102;   9 docs/s;  89416 sec
[2024-04-16 18:17:20,500 INFO] Step 38350/50000; xent: 2.65; lr: 0.0000102;   9 docs/s;  89532 sec
[2024-04-16 18:19:16,295 INFO] Step 38400/50000; xent: 2.64; lr: 0.0000102;   9 docs/s;  89648 sec
[2024-04-16 18:20:33,463 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.35.bert.pt, number of examples: 1998
[2024-04-16 18:21:11,940 INFO] Step 38450/50000; xent: 2.60; lr: 0.0000102;   9 docs/s;  89763 sec
[2024-04-16 18:23:12,747 INFO] Step 38500/50000; xent: 2.65; lr: 0.0000102;   9 docs/s;  89884 sec
[2024-04-16 18:25:09,599 INFO] Step 38550/50000; xent: 2.55; lr: 0.0000102;   9 docs/s;  90001 sec
[2024-04-16 18:27:07,930 INFO] Step 38600/50000; xent: 2.64; lr: 0.0000102;   9 docs/s;  90119 sec
[2024-04-16 18:28:03,432 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.104.bert.pt, number of examples: 2000
[2024-04-16 18:29:06,379 INFO] Step 38650/50000; xent: 2.63; lr: 0.0000102;   9 docs/s;  90238 sec
[2024-04-16 18:31:02,071 INFO] Step 38700/50000; xent: 2.61; lr: 0.0000102;   9 docs/s;  90354 sec
[2024-04-16 18:32:59,832 INFO] Step 38750/50000; xent: 2.66; lr: 0.0000102;   9 docs/s;  90471 sec
[2024-04-16 18:34:57,119 INFO] Step 38800/50000; xent: 2.59; lr: 0.0000102;   9 docs/s;  90589 sec
[2024-04-16 18:35:29,538 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.0.bert.pt, number of examples: 2001
[2024-04-16 18:36:53,518 INFO] Step 38850/50000; xent: 2.68; lr: 0.0000101;   9 docs/s;  90705 sec
[2024-04-16 18:38:50,175 INFO] Step 38900/50000; xent: 2.65; lr: 0.0000101;   9 docs/s;  90822 sec
[2024-04-16 18:40:49,393 INFO] Step 38950/50000; xent: 2.67; lr: 0.0000101;   9 docs/s;  90941 sec
[2024-04-16 18:42:46,799 INFO] Step 39000/50000; xent: 2.65; lr: 0.0000101;   9 docs/s;  91058 sec
[2024-04-16 18:42:46,800 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_39000.pt
[2024-04-16 18:43:01,864 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.69.bert.pt, number of examples: 2000
[2024-04-16 18:44:47,165 INFO] Step 39050/50000; xent: 2.68; lr: 0.0000101;   9 docs/s;  91179 sec
[2024-04-16 18:46:44,520 INFO] Step 39100/50000; xent: 2.64; lr: 0.0000101;   9 docs/s;  91296 sec
[2024-04-16 18:48:41,099 INFO] Step 39150/50000; xent: 2.62; lr: 0.0000101;   9 docs/s;  91413 sec
[2024-04-16 18:50:29,313 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.15.bert.pt, number of examples: 1999
[2024-04-16 18:50:38,622 INFO] Step 39200/50000; xent: 2.71; lr: 0.0000101;   9 docs/s;  91530 sec
[2024-04-16 18:52:35,782 INFO] Step 39250/50000; xent: 2.68; lr: 0.0000101;   9 docs/s;  91647 sec
[2024-04-16 18:54:32,153 INFO] Step 39300/50000; xent: 2.74; lr: 0.0000101;   9 docs/s;  91764 sec
[2024-04-16 18:56:30,237 INFO] Step 39350/50000; xent: 2.71; lr: 0.0000101;   9 docs/s;  91882 sec
[2024-04-16 18:57:52,935 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.142.bert.pt, number of examples: 2000
[2024-04-16 18:58:25,481 INFO] Step 39400/50000; xent: 2.63; lr: 0.0000101;   9 docs/s;  91997 sec
[2024-04-16 19:00:22,900 INFO] Step 39450/50000; xent: 2.57; lr: 0.0000101;   9 docs/s;  92114 sec
[2024-04-16 19:02:22,885 INFO] Step 39500/50000; xent: 2.60; lr: 0.0000101;   9 docs/s;  92234 sec
[2024-04-16 19:04:20,564 INFO] Step 39550/50000; xent: 2.69; lr: 0.0000101;   9 docs/s;  92352 sec
[2024-04-16 19:05:25,115 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.20.bert.pt, number of examples: 2000
[2024-04-16 19:06:20,290 INFO] Step 39600/50000; xent: 2.79; lr: 0.0000101;   9 docs/s;  92472 sec
[2024-04-16 19:08:16,520 INFO] Step 39650/50000; xent: 2.61; lr: 0.0000100;   9 docs/s;  92588 sec
[2024-04-16 19:10:13,177 INFO] Step 39700/50000; xent: 2.65; lr: 0.0000100;   9 docs/s;  92705 sec
[2024-04-16 19:12:10,008 INFO] Step 39750/50000; xent: 2.61; lr: 0.0000100;   9 docs/s;  92821 sec
[2024-04-16 19:12:49,062 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.108.bert.pt, number of examples: 2000
[2024-04-16 19:14:05,232 INFO] Step 39800/50000; xent: 2.63; lr: 0.0000100;   9 docs/s;  92937 sec
[2024-04-16 19:16:02,583 INFO] Step 39850/50000; xent: 2.61; lr: 0.0000100;   9 docs/s;  93054 sec
[2024-04-16 19:17:59,987 INFO] Step 39900/50000; xent: 2.60; lr: 0.0000100;   9 docs/s;  93171 sec
[2024-04-16 19:19:57,398 INFO] Step 39950/50000; xent: 2.60; lr: 0.0000100;   9 docs/s;  93289 sec
[2024-04-16 19:20:13,940 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.81.bert.pt, number of examples: 2000
[2024-04-16 19:21:54,331 INFO] Step 40000/50000; xent: 2.70; lr: 0.0000100;   9 docs/s;  93406 sec
[2024-04-16 19:21:54,341 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_40000.pt
[2024-04-16 19:23:54,399 INFO] Step 40050/50000; xent: 2.70; lr: 0.0000100;   9 docs/s;  93526 sec
[2024-04-16 19:25:52,050 INFO] Step 40100/50000; xent: 2.66; lr: 0.0000100;   9 docs/s;  93643 sec
[2024-04-16 19:27:40,690 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.89.bert.pt, number of examples: 2001
[2024-04-16 19:27:50,101 INFO] Step 40150/50000; xent: 2.64; lr: 0.0000100;   9 docs/s;  93762 sec
[2024-04-16 19:29:46,973 INFO] Step 40200/50000; xent: 2.56; lr: 0.0000100;   9 docs/s;  93878 sec
[2024-04-16 19:31:45,632 INFO] Step 40250/50000; xent: 2.67; lr: 0.0000100;   9 docs/s;  93997 sec
[2024-04-16 19:33:44,583 INFO] Step 40300/50000; xent: 2.56; lr: 0.0000100;   9 docs/s;  94116 sec
[2024-04-16 19:35:07,003 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.74.bert.pt, number of examples: 2000
[2024-04-16 19:35:42,261 INFO] Step 40350/50000; xent: 2.63; lr: 0.0000100;   9 docs/s;  94234 sec
[2024-04-16 19:37:38,971 INFO] Step 40400/50000; xent: 2.51; lr: 0.0000100;   9 docs/s;  94350 sec
[2024-04-16 19:39:37,823 INFO] Step 40450/50000; xent: 2.57; lr: 0.0000099;   9 docs/s;  94469 sec
[2024-04-16 19:41:34,560 INFO] Step 40500/50000; xent: 2.69; lr: 0.0000099;   9 docs/s;  94586 sec
[2024-04-16 19:42:35,331 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.77.bert.pt, number of examples: 2001
[2024-04-16 19:43:30,925 INFO] Step 40550/50000; xent: 2.56; lr: 0.0000099;   9 docs/s;  94702 sec
[2024-04-16 19:45:28,938 INFO] Step 40600/50000; xent: 2.63; lr: 0.0000099;   9 docs/s;  94820 sec
[2024-04-16 19:47:26,439 INFO] Step 40650/50000; xent: 2.67; lr: 0.0000099;   9 docs/s;  94938 sec
[2024-04-16 19:49:23,426 INFO] Step 40700/50000; xent: 2.66; lr: 0.0000099;   9 docs/s;  95055 sec
[2024-04-16 19:50:02,990 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.125.bert.pt, number of examples: 2000
[2024-04-16 19:51:20,734 INFO] Step 40750/50000; xent: 2.65; lr: 0.0000099;   9 docs/s;  95172 sec
[2024-04-16 19:53:18,811 INFO] Step 40800/50000; xent: 2.59; lr: 0.0000099;   9 docs/s;  95290 sec
[2024-04-16 19:55:16,448 INFO] Step 40850/50000; xent: 2.67; lr: 0.0000099;   9 docs/s;  95408 sec
[2024-04-16 19:57:13,228 INFO] Step 40900/50000; xent: 2.68; lr: 0.0000099;   9 docs/s;  95525 sec
[2024-04-16 19:57:32,682 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.134.bert.pt, number of examples: 2001
[2024-04-16 19:59:10,737 INFO] Step 40950/50000; xent: 2.64; lr: 0.0000099;   9 docs/s;  95642 sec
[2024-04-16 20:01:09,175 INFO] Step 41000/50000; xent: 2.65; lr: 0.0000099;   9 docs/s;  95761 sec
[2024-04-16 20:01:09,185 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_41000.pt
[2024-04-16 20:03:10,069 INFO] Step 41050/50000; xent: 2.65; lr: 0.0000099;   9 docs/s;  95882 sec
[2024-04-16 20:05:02,095 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.1.bert.pt, number of examples: 2001
[2024-04-16 20:05:06,780 INFO] Step 41100/50000; xent: 2.58; lr: 0.0000099;   9 docs/s;  95998 sec
[2024-04-16 20:07:03,466 INFO] Step 41150/50000; xent: 2.56; lr: 0.0000099;   9 docs/s;  96115 sec
[2024-04-16 20:09:00,926 INFO] Step 41200/50000; xent: 2.67; lr: 0.0000099;   9 docs/s;  96232 sec
[2024-04-16 20:10:59,943 INFO] Step 41250/50000; xent: 2.60; lr: 0.0000098;   9 docs/s;  96351 sec
[2024-04-16 20:12:29,198 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.80.bert.pt, number of examples: 1999
[2024-04-16 20:12:57,344 INFO] Step 41300/50000; xent: 2.62; lr: 0.0000098;   9 docs/s;  96469 sec
[2024-04-16 20:14:55,226 INFO] Step 41350/50000; xent: 2.71; lr: 0.0000098;   9 docs/s;  96587 sec
[2024-04-16 20:16:51,888 INFO] Step 41400/50000; xent: 2.67; lr: 0.0000098;   9 docs/s;  96703 sec
[2024-04-16 20:18:49,110 INFO] Step 41450/50000; xent: 2.60; lr: 0.0000098;   9 docs/s;  96821 sec
[2024-04-16 20:19:52,011 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.139.bert.pt, number of examples: 2001
[2024-04-16 20:20:46,866 INFO] Step 41500/50000; xent: 2.67; lr: 0.0000098;   9 docs/s;  96938 sec
[2024-04-16 20:22:45,584 INFO] Step 41550/50000; xent: 2.60; lr: 0.0000098;   9 docs/s;  97057 sec
[2024-04-16 20:24:42,718 INFO] Step 41600/50000; xent: 2.65; lr: 0.0000098;   9 docs/s;  97174 sec
[2024-04-16 20:26:40,522 INFO] Step 41650/50000; xent: 2.67; lr: 0.0000098;   9 docs/s;  97292 sec
[2024-04-16 20:27:21,901 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.82.bert.pt, number of examples: 2001
[2024-04-16 20:28:36,277 INFO] Step 41700/50000; xent: 2.70; lr: 0.0000098;   9 docs/s;  97408 sec
[2024-04-16 20:30:36,735 INFO] Step 41750/50000; xent: 2.76; lr: 0.0000098;   9 docs/s;  97528 sec
[2024-04-16 20:32:35,069 INFO] Step 41800/50000; xent: 2.72; lr: 0.0000098;   9 docs/s;  97647 sec
[2024-04-16 20:34:32,251 INFO] Step 41850/50000; xent: 2.74; lr: 0.0000098;   9 docs/s;  97764 sec
[2024-04-16 20:34:49,118 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.101.bert.pt, number of examples: 2000
[2024-04-16 20:36:30,804 INFO] Step 41900/50000; xent: 2.71; lr: 0.0000098;   9 docs/s;  97882 sec
[2024-04-16 20:38:29,975 INFO] Step 41950/50000; xent: 2.53; lr: 0.0000098;   9 docs/s;  98001 sec
[2024-04-16 20:40:25,164 INFO] Step 42000/50000; xent: 2.56; lr: 0.0000098;   9 docs/s;  98117 sec
[2024-04-16 20:40:25,174 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_42000.pt
[2024-04-16 20:42:21,226 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.3.bert.pt, number of examples: 2001
[2024-04-16 20:42:25,857 INFO] Step 42050/50000; xent: 2.62; lr: 0.0000098;   9 docs/s;  98237 sec
[2024-04-16 20:44:24,685 INFO] Step 42100/50000; xent: 2.56; lr: 0.0000097;   9 docs/s;  98356 sec
[2024-04-16 20:46:20,941 INFO] Step 42150/50000; xent: 2.60; lr: 0.0000097;   9 docs/s;  98472 sec
[2024-04-16 20:48:16,758 INFO] Step 42200/50000; xent: 2.61; lr: 0.0000097;   9 docs/s;  98588 sec
[2024-04-16 20:49:48,351 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.122.bert.pt, number of examples: 2000
[2024-04-16 20:50:13,939 INFO] Step 42250/50000; xent: 2.67; lr: 0.0000097;   9 docs/s;  98705 sec
[2024-04-16 20:52:10,793 INFO] Step 42300/50000; xent: 2.63; lr: 0.0000097;   9 docs/s;  98822 sec
[2024-04-16 20:54:08,150 INFO] Step 42350/50000; xent: 2.65; lr: 0.0000097;   9 docs/s;  98940 sec
[2024-04-16 20:56:05,050 INFO] Step 42400/50000; xent: 2.62; lr: 0.0000097;   9 docs/s;  99056 sec
[2024-04-16 20:57:12,606 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.68.bert.pt, number of examples: 1999
[2024-04-16 20:58:02,242 INFO] Step 42450/50000; xent: 2.61; lr: 0.0000097;   9 docs/s;  99174 sec
[2024-04-16 21:00:00,869 INFO] Step 42500/50000; xent: 2.61; lr: 0.0000097;   9 docs/s;  99292 sec
[2024-04-16 21:01:59,242 INFO] Step 42550/50000; xent: 2.58; lr: 0.0000097;   9 docs/s;  99411 sec
[2024-04-16 21:03:58,511 INFO] Step 42600/50000; xent: 2.66; lr: 0.0000097;   9 docs/s;  99530 sec
[2024-04-16 21:04:42,025 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.54.bert.pt, number of examples: 2001
[2024-04-16 21:05:54,804 INFO] Step 42650/50000; xent: 2.64; lr: 0.0000097;   9 docs/s;  99646 sec
[2024-04-16 21:07:53,223 INFO] Step 42700/50000; xent: 2.68; lr: 0.0000097;   9 docs/s;  99765 sec
[2024-04-16 21:09:52,123 INFO] Step 42750/50000; xent: 2.73; lr: 0.0000097;   9 docs/s;  99884 sec
[2024-04-16 21:11:48,300 INFO] Step 42800/50000; xent: 2.76; lr: 0.0000097;   9 docs/s; 100000 sec
[2024-04-16 21:12:12,049 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.87.bert.pt, number of examples: 1999
[2024-04-16 21:13:46,199 INFO] Step 42850/50000; xent: 2.65; lr: 0.0000097;   9 docs/s; 100118 sec
[2024-04-16 21:15:43,493 INFO] Step 42900/50000; xent: 2.59; lr: 0.0000097;   9 docs/s; 100235 sec
[2024-04-16 21:17:42,705 INFO] Step 42950/50000; xent: 2.69; lr: 0.0000097;   9 docs/s; 100354 sec
[2024-04-16 21:19:42,063 INFO] Step 43000/50000; xent: 2.56; lr: 0.0000096;   9 docs/s; 100474 sec
[2024-04-16 21:19:42,065 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_43000.pt
[2024-04-16 21:19:47,649 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.50.bert.pt, number of examples: 2001
[2024-04-16 21:21:42,400 INFO] Step 43050/50000; xent: 2.61; lr: 0.0000096;   9 docs/s; 100594 sec
[2024-04-16 21:23:39,735 INFO] Step 43100/50000; xent: 2.54; lr: 0.0000096;   9 docs/s; 100711 sec
[2024-04-16 21:25:38,420 INFO] Step 43150/50000; xent: 2.66; lr: 0.0000096;   9 docs/s; 100830 sec
[2024-04-16 21:27:14,993 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.75.bert.pt, number of examples: 1999
[2024-04-16 21:27:36,318 INFO] Step 43200/50000; xent: 2.69; lr: 0.0000096;   9 docs/s; 100948 sec
[2024-04-16 21:29:33,260 INFO] Step 43250/50000; xent: 2.56; lr: 0.0000096;   9 docs/s; 101065 sec
[2024-04-16 21:31:32,232 INFO] Step 43300/50000; xent: 2.61; lr: 0.0000096;   9 docs/s; 101184 sec
[2024-04-16 21:33:30,087 INFO] Step 43350/50000; xent: 2.59; lr: 0.0000096;   9 docs/s; 101302 sec
[2024-04-16 21:34:44,999 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.141.bert.pt, number of examples: 1999
[2024-04-16 21:35:27,745 INFO] Step 43400/50000; xent: 2.61; lr: 0.0000096;   9 docs/s; 101419 sec
[2024-04-16 21:37:25,501 INFO] Step 43450/50000; xent: 2.65; lr: 0.0000096;   9 docs/s; 101537 sec
[2024-04-16 21:39:22,719 INFO] Step 43500/50000; xent: 2.57; lr: 0.0000096;   9 docs/s; 101654 sec
[2024-04-16 21:41:19,147 INFO] Step 43550/50000; xent: 2.60; lr: 0.0000096;   9 docs/s; 101771 sec
[2024-04-16 21:42:10,456 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.51.bert.pt, number of examples: 2000
[2024-04-16 21:43:18,062 INFO] Step 43600/50000; xent: 2.61; lr: 0.0000096;   9 docs/s; 101890 sec
[2024-04-16 21:45:18,488 INFO] Step 43650/50000; xent: 2.73; lr: 0.0000096;   9 docs/s; 102010 sec
[2024-04-16 21:47:15,117 INFO] Step 43700/50000; xent: 2.77; lr: 0.0000096;   9 docs/s; 102127 sec
[2024-04-16 21:49:12,701 INFO] Step 43750/50000; xent: 2.69; lr: 0.0000096;   9 docs/s; 102244 sec
[2024-04-16 21:49:39,806 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.76.bert.pt, number of examples: 1999
[2024-04-16 21:51:11,076 INFO] Step 43800/50000; xent: 2.61; lr: 0.0000096;   9 docs/s; 102363 sec
[2024-04-16 21:53:08,598 INFO] Step 43850/50000; xent: 2.63; lr: 0.0000096;   9 docs/s; 102480 sec
[2024-04-16 21:55:05,755 INFO] Step 43900/50000; xent: 2.55; lr: 0.0000095;   9 docs/s; 102597 sec
[2024-04-16 21:57:01,617 INFO] Step 43950/50000; xent: 2.59; lr: 0.0000095;   9 docs/s; 102713 sec
[2024-04-16 21:57:06,496 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.118.bert.pt, number of examples: 2001
[2024-04-16 21:58:59,832 INFO] Step 44000/50000; xent: 2.64; lr: 0.0000095;   9 docs/s; 102831 sec
[2024-04-16 21:58:59,842 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_44000.pt
[2024-04-16 22:01:00,561 INFO] Step 44050/50000; xent: 2.56; lr: 0.0000095;   9 docs/s; 102952 sec
[2024-04-16 22:02:57,607 INFO] Step 44100/50000; xent: 2.58; lr: 0.0000095;   9 docs/s; 103069 sec
[2024-04-16 22:04:37,189 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.138.bert.pt, number of examples: 2000
[2024-04-16 22:04:55,856 INFO] Step 44150/50000; xent: 2.60; lr: 0.0000095;   9 docs/s; 103187 sec
[2024-04-16 22:06:53,503 INFO] Step 44200/50000; xent: 2.67; lr: 0.0000095;   9 docs/s; 103305 sec
[2024-04-16 22:08:50,525 INFO] Step 44250/50000; xent: 2.52; lr: 0.0000095;   9 docs/s; 103422 sec
[2024-04-16 22:10:48,461 INFO] Step 44300/50000; xent: 2.65; lr: 0.0000095;   9 docs/s; 103540 sec
[2024-04-16 22:12:05,558 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.140.bert.pt, number of examples: 2000
[2024-04-16 22:12:44,808 INFO] Step 44350/50000; xent: 2.59; lr: 0.0000095;   9 docs/s; 103656 sec
[2024-04-16 22:14:42,725 INFO] Step 44400/50000; xent: 2.66; lr: 0.0000095;   9 docs/s; 103774 sec
[2024-04-16 22:16:40,729 INFO] Step 44450/50000; xent: 2.68; lr: 0.0000095;   9 docs/s; 103892 sec
[2024-04-16 22:18:38,515 INFO] Step 44500/50000; xent: 2.58; lr: 0.0000095;   9 docs/s; 104010 sec
[2024-04-16 22:19:32,696 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.112.bert.pt, number of examples: 1999
[2024-04-16 22:20:36,416 INFO] Step 44550/50000; xent: 2.64; lr: 0.0000095;   9 docs/s; 104128 sec
[2024-04-16 22:22:34,242 INFO] Step 44600/50000; xent: 2.71; lr: 0.0000095;   9 docs/s; 104246 sec
[2024-04-16 22:24:32,441 INFO] Step 44650/50000; xent: 2.70; lr: 0.0000095;   9 docs/s; 104364 sec
[2024-04-16 22:26:29,254 INFO] Step 44700/50000; xent: 2.62; lr: 0.0000095;   9 docs/s; 104481 sec
[2024-04-16 22:27:00,330 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.57.bert.pt, number of examples: 1999
[2024-04-16 22:28:29,902 INFO] Step 44750/50000; xent: 2.67; lr: 0.0000095;   9 docs/s; 104601 sec
[2024-04-16 22:30:26,270 INFO] Step 44800/50000; xent: 2.59; lr: 0.0000094;   9 docs/s; 104718 sec
[2024-04-16 22:32:23,463 INFO] Step 44850/50000; xent: 2.62; lr: 0.0000094;   9 docs/s; 104835 sec
[2024-04-16 22:34:20,968 INFO] Step 44900/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 104952 sec
[2024-04-16 22:34:28,172 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.7.bert.pt, number of examples: 2001
[2024-04-16 22:36:18,173 INFO] Step 44950/50000; xent: 2.62; lr: 0.0000094;   9 docs/s; 105070 sec
[2024-04-16 22:38:14,894 INFO] Step 45000/50000; xent: 2.58; lr: 0.0000094;   9 docs/s; 105186 sec
[2024-04-16 22:38:14,905 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_45000.pt
[2024-04-16 22:40:16,001 INFO] Step 45050/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 105307 sec
[2024-04-16 22:41:58,934 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.127.bert.pt, number of examples: 2000
[2024-04-16 22:42:12,952 INFO] Step 45100/50000; xent: 2.68; lr: 0.0000094;   9 docs/s; 105424 sec
[2024-04-16 22:44:08,653 INFO] Step 45150/50000; xent: 2.64; lr: 0.0000094;   9 docs/s; 105540 sec
[2024-04-16 22:46:07,063 INFO] Step 45200/50000; xent: 2.64; lr: 0.0000094;   9 docs/s; 105659 sec
[2024-04-16 22:48:05,164 INFO] Step 45250/50000; xent: 2.58; lr: 0.0000094;   9 docs/s; 105777 sec
[2024-04-16 22:49:26,434 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.27.bert.pt, number of examples: 2001
[2024-04-16 22:50:02,289 INFO] Step 45300/50000; xent: 2.60; lr: 0.0000094;   9 docs/s; 105894 sec
[2024-04-16 22:52:01,488 INFO] Step 45350/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 106013 sec
[2024-04-16 22:53:58,946 INFO] Step 45400/50000; xent: 2.69; lr: 0.0000094;   9 docs/s; 106130 sec
[2024-04-16 22:56:01,123 INFO] Step 45450/50000; xent: 2.61; lr: 0.0000094;   9 docs/s; 106253 sec
[2024-04-16 22:57:01,304 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.71.bert.pt, number of examples: 1999
[2024-04-16 22:58:00,817 INFO] Step 45500/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 106372 sec
[2024-04-16 23:00:01,773 INFO] Step 45550/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 106493 sec
[2024-04-16 23:02:02,305 INFO] Step 45600/50000; xent: 2.71; lr: 0.0000094;   9 docs/s; 106614 sec
[2024-04-16 23:04:01,062 INFO] Step 45650/50000; xent: 2.69; lr: 0.0000094;   9 docs/s; 106733 sec
[2024-04-16 23:04:39,931 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.40.bert.pt, number of examples: 1999
[2024-04-16 23:06:01,213 INFO] Step 45700/50000; xent: 2.63; lr: 0.0000094;   9 docs/s; 106853 sec
[2024-04-16 23:08:01,300 INFO] Step 45750/50000; xent: 2.65; lr: 0.0000094;   9 docs/s; 106973 sec
[2024-04-16 23:10:03,116 INFO] Step 45800/50000; xent: 2.59; lr: 0.0000093;   9 docs/s; 107095 sec
[2024-04-16 23:12:02,294 INFO] Step 45850/50000; xent: 2.60; lr: 0.0000093;   9 docs/s; 107214 sec
[2024-04-16 23:12:13,802 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.8.bert.pt, number of examples: 2000
[2024-04-16 23:14:00,149 INFO] Step 45900/50000; xent: 2.67; lr: 0.0000093;   9 docs/s; 107332 sec
[2024-04-16 23:15:58,399 INFO] Step 45950/50000; xent: 2.55; lr: 0.0000093;   9 docs/s; 107450 sec
[2024-04-16 23:17:56,737 INFO] Step 46000/50000; xent: 2.67; lr: 0.0000093;   9 docs/s; 107568 sec
[2024-04-16 23:17:56,746 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_46000.pt
[2024-04-16 23:19:43,020 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.79.bert.pt, number of examples: 1999
[2024-04-16 23:19:57,007 INFO] Step 46050/50000; xent: 2.64; lr: 0.0000093;   9 docs/s; 107688 sec
[2024-04-16 23:21:56,101 INFO] Step 46100/50000; xent: 2.64; lr: 0.0000093;   9 docs/s; 107808 sec
[2024-04-16 23:23:53,999 INFO] Step 46150/50000; xent: 2.62; lr: 0.0000093;   9 docs/s; 107925 sec
[2024-04-16 23:25:50,868 INFO] Step 46200/50000; xent: 2.66; lr: 0.0000093;   9 docs/s; 108042 sec
[2024-04-16 23:27:17,070 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.105.bert.pt, number of examples: 2001
[2024-04-16 23:27:52,068 INFO] Step 46250/50000; xent: 2.61; lr: 0.0000093;   9 docs/s; 108164 sec
[2024-04-16 23:29:48,500 INFO] Step 46300/50000; xent: 2.61; lr: 0.0000093;   9 docs/s; 108280 sec
[2024-04-16 23:31:48,055 INFO] Step 46350/50000; xent: 2.76; lr: 0.0000093;   9 docs/s; 108399 sec
[2024-04-16 23:33:45,975 INFO] Step 46400/50000; xent: 2.73; lr: 0.0000093;   9 docs/s; 108517 sec
[2024-04-16 23:34:45,004 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.126.bert.pt, number of examples: 1999
[2024-04-16 23:35:43,825 INFO] Step 46450/50000; xent: 2.65; lr: 0.0000093;   9 docs/s; 108635 sec
[2024-04-16 23:37:42,456 INFO] Step 46500/50000; xent: 2.64; lr: 0.0000093;   9 docs/s; 108754 sec
[2024-04-16 23:39:39,880 INFO] Step 46550/50000; xent: 2.70; lr: 0.0000093;   9 docs/s; 108871 sec
[2024-04-16 23:41:37,658 INFO] Step 46600/50000; xent: 2.59; lr: 0.0000093;   9 docs/s; 108989 sec
[2024-04-16 23:42:08,442 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.98.bert.pt, number of examples: 2000
[2024-04-16 23:43:35,880 INFO] Step 46650/50000; xent: 2.65; lr: 0.0000093;   9 docs/s; 109107 sec
[2024-04-16 23:45:35,254 INFO] Step 46700/50000; xent: 2.66; lr: 0.0000093;   9 docs/s; 109227 sec
[2024-04-16 23:47:32,450 INFO] Step 46750/50000; xent: 2.72; lr: 0.0000092;   9 docs/s; 109344 sec
[2024-04-16 23:49:29,282 INFO] Step 46800/50000; xent: 2.58; lr: 0.0000092;   9 docs/s; 109461 sec
[2024-04-16 23:49:38,188 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.63.bert.pt, number of examples: 2001
[2024-04-16 23:51:28,105 INFO] Step 46850/50000; xent: 2.64; lr: 0.0000092;   9 docs/s; 109580 sec
[2024-04-16 23:53:25,867 INFO] Step 46900/50000; xent: 2.57; lr: 0.0000092;   9 docs/s; 109697 sec
[2024-04-16 23:55:22,428 INFO] Step 46950/50000; xent: 2.69; lr: 0.0000092;   9 docs/s; 109814 sec
[2024-04-16 23:57:03,564 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.61.bert.pt, number of examples: 2001
[2024-04-16 23:57:20,279 INFO] Step 47000/50000; xent: 2.59; lr: 0.0000092;   9 docs/s; 109932 sec
[2024-04-16 23:57:20,290 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_47000.pt
[2024-04-16 23:59:19,587 INFO] Step 47050/50000; xent: 2.62; lr: 0.0000092;   9 docs/s; 110051 sec
[2024-04-17 00:01:17,010 INFO] Step 47100/50000; xent: 2.64; lr: 0.0000092;   9 docs/s; 110168 sec
[2024-04-17 00:03:14,878 INFO] Step 47150/50000; xent: 2.56; lr: 0.0000092;   9 docs/s; 110286 sec
[2024-04-17 00:04:34,222 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.100.bert.pt, number of examples: 1999
[2024-04-17 00:05:14,174 INFO] Step 47200/50000; xent: 2.68; lr: 0.0000092;   9 docs/s; 110406 sec
[2024-04-17 00:07:11,065 INFO] Step 47250/50000; xent: 2.56; lr: 0.0000092;   9 docs/s; 110523 sec
[2024-04-17 00:09:12,819 INFO] Step 47300/50000; xent: 2.60; lr: 0.0000092;   9 docs/s; 110644 sec
[2024-04-17 00:11:14,793 INFO] Step 47350/50000; xent: 2.69; lr: 0.0000092;   9 docs/s; 110766 sec
[2024-04-17 00:12:10,539 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.84.bert.pt, number of examples: 2000
[2024-04-17 00:13:12,396 INFO] Step 47400/50000; xent: 2.61; lr: 0.0000092;   9 docs/s; 110884 sec
[2024-04-17 00:15:11,051 INFO] Step 47450/50000; xent: 2.71; lr: 0.0000092;   9 docs/s; 111002 sec
[2024-04-17 00:17:07,568 INFO] Step 47500/50000; xent: 2.73; lr: 0.0000092;   9 docs/s; 111119 sec
[2024-04-17 00:19:06,186 INFO] Step 47550/50000; xent: 2.73; lr: 0.0000092;   9 docs/s; 111238 sec
[2024-04-17 00:19:40,989 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.111.bert.pt, number of examples: 2000
[2024-04-17 00:21:04,305 INFO] Step 47600/50000; xent: 2.70; lr: 0.0000092;   9 docs/s; 111356 sec
[2024-04-17 00:23:01,133 INFO] Step 47650/50000; xent: 2.70; lr: 0.0000092;   9 docs/s; 111473 sec
[2024-04-17 00:24:58,640 INFO] Step 47700/50000; xent: 2.72; lr: 0.0000092;   9 docs/s; 111590 sec
[2024-04-17 00:27:02,707 INFO] Step 47750/50000; xent: 2.72; lr: 0.0000092;   8 docs/s; 111714 sec
[2024-04-17 00:27:14,711 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.56.bert.pt, number of examples: 2001
[2024-04-17 00:29:01,596 INFO] Step 47800/50000; xent: 2.83; lr: 0.0000091;   9 docs/s; 111833 sec
[2024-04-17 00:30:58,028 INFO] Step 47850/50000; xent: 2.67; lr: 0.0000091;   9 docs/s; 111949 sec
[2024-04-17 00:32:56,284 INFO] Step 47900/50000; xent: 2.69; lr: 0.0000091;   9 docs/s; 112068 sec
[2024-04-17 00:34:42,390 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.31.bert.pt, number of examples: 2000
[2024-04-17 00:34:54,045 INFO] Step 47950/50000; xent: 2.68; lr: 0.0000091;   9 docs/s; 112185 sec
[2024-04-17 00:36:51,217 INFO] Step 48000/50000; xent: 2.64; lr: 0.0000091;   9 docs/s; 112303 sec
[2024-04-17 00:36:51,228 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_48000.pt
[2024-04-17 00:38:55,036 INFO] Step 48050/50000; xent: 2.60; lr: 0.0000091;   8 docs/s; 112426 sec
[2024-04-17 00:40:51,646 INFO] Step 48100/50000; xent: 2.61; lr: 0.0000091;   9 docs/s; 112543 sec
[2024-04-17 00:42:15,171 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.110.bert.pt, number of examples: 2000
[2024-04-17 00:42:47,724 INFO] Step 48150/50000; xent: 2.59; lr: 0.0000091;   9 docs/s; 112659 sec
[2024-04-17 00:44:44,128 INFO] Step 48200/50000; xent: 2.61; lr: 0.0000091;   9 docs/s; 112776 sec
[2024-04-17 00:46:42,368 INFO] Step 48250/50000; xent: 2.66; lr: 0.0000091;   9 docs/s; 112894 sec
[2024-04-17 00:48:39,244 INFO] Step 48300/50000; xent: 2.62; lr: 0.0000091;   9 docs/s; 113011 sec
[2024-04-17 00:49:38,234 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.93.bert.pt, number of examples: 1997
[2024-04-17 00:50:36,556 INFO] Step 48350/50000; xent: 2.66; lr: 0.0000091;   9 docs/s; 113128 sec
[2024-04-17 00:52:33,750 INFO] Step 48400/50000; xent: 2.59; lr: 0.0000091;   9 docs/s; 113245 sec
[2024-04-17 00:54:30,854 INFO] Step 48450/50000; xent: 2.58; lr: 0.0000091;   9 docs/s; 113362 sec
[2024-04-17 00:56:27,588 INFO] Step 48500/50000; xent: 2.71; lr: 0.0000091;   9 docs/s; 113479 sec
[2024-04-17 00:57:05,569 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.88.bert.pt, number of examples: 1999
[2024-04-17 00:58:27,290 INFO] Step 48550/50000; xent: 2.69; lr: 0.0000091;   9 docs/s; 113599 sec
[2024-04-17 01:00:23,797 INFO] Step 48600/50000; xent: 2.65; lr: 0.0000091;   9 docs/s; 113715 sec
[2024-04-17 01:02:21,488 INFO] Step 48650/50000; xent: 2.74; lr: 0.0000091;   9 docs/s; 113833 sec
[2024-04-17 01:04:18,985 INFO] Step 48700/50000; xent: 2.71; lr: 0.0000091;   9 docs/s; 113950 sec
[2024-04-17 01:04:35,861 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.22.bert.pt, number of examples: 1999
[2024-04-17 01:06:18,104 INFO] Step 48750/50000; xent: 2.54; lr: 0.0000091;   9 docs/s; 114070 sec
[2024-04-17 01:08:15,044 INFO] Step 48800/50000; xent: 2.70; lr: 0.0000091;   9 docs/s; 114186 sec
[2024-04-17 01:10:14,187 INFO] Step 48850/50000; xent: 2.66; lr: 0.0000090;   9 docs/s; 114306 sec
[2024-04-17 01:12:03,834 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.16.bert.pt, number of examples: 2001
[2024-04-17 01:12:11,186 INFO] Step 48900/50000; xent: 2.62; lr: 0.0000090;   9 docs/s; 114423 sec
[2024-04-17 01:14:08,445 INFO] Step 48950/50000; xent: 2.66; lr: 0.0000090;   9 docs/s; 114540 sec
[2024-04-17 01:16:04,716 INFO] Step 49000/50000; xent: 2.71; lr: 0.0000090;   9 docs/s; 114656 sec
[2024-04-17 01:16:04,726 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_49000.pt
[2024-04-17 01:18:07,235 INFO] Step 49050/50000; xent: 2.70; lr: 0.0000090;   9 docs/s; 114779 sec
[2024-04-17 01:19:34,409 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.37.bert.pt, number of examples: 1999
[2024-04-17 01:20:04,463 INFO] Step 49100/50000; xent: 2.74; lr: 0.0000090;   9 docs/s; 114896 sec
[2024-04-17 01:22:05,220 INFO] Step 49150/50000; xent: 2.64; lr: 0.0000090;   9 docs/s; 115017 sec
[2024-04-17 01:24:02,061 INFO] Step 49200/50000; xent: 2.52; lr: 0.0000090;   9 docs/s; 115134 sec
[2024-04-17 01:25:59,391 INFO] Step 49250/50000; xent: 2.68; lr: 0.0000090;   9 docs/s; 115251 sec
[2024-04-17 01:27:00,764 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.92.bert.pt, number of examples: 2000
[2024-04-17 01:27:57,253 INFO] Step 49300/50000; xent: 2.64; lr: 0.0000090;   9 docs/s; 115369 sec
[2024-04-17 01:29:55,077 INFO] Step 49350/50000; xent: 2.65; lr: 0.0000090;   9 docs/s; 115487 sec
[2024-04-17 01:31:52,935 INFO] Step 49400/50000; xent: 2.59; lr: 0.0000090;   9 docs/s; 115604 sec
[2024-04-17 01:33:50,411 INFO] Step 49450/50000; xent: 2.60; lr: 0.0000090;   9 docs/s; 115722 sec
[2024-04-17 01:34:28,063 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.123.bert.pt, number of examples: 2001
[2024-04-17 01:35:48,675 INFO] Step 49500/50000; xent: 2.58; lr: 0.0000090;   9 docs/s; 115840 sec
[2024-04-17 01:37:47,510 INFO] Step 49550/50000; xent: 2.71; lr: 0.0000090;   9 docs/s; 115959 sec
[2024-04-17 01:39:45,052 INFO] Step 49600/50000; xent: 2.61; lr: 0.0000090;   9 docs/s; 116076 sec
[2024-04-17 01:41:41,402 INFO] Step 49650/50000; xent: 2.58; lr: 0.0000090;   9 docs/s; 116193 sec
[2024-04-17 01:41:58,055 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.121.bert.pt, number of examples: 2001
[2024-04-17 01:43:40,223 INFO] Step 49700/50000; xent: 2.45; lr: 0.0000090;   9 docs/s; 116312 sec
[2024-04-17 01:45:37,788 INFO] Step 49750/50000; xent: 2.66; lr: 0.0000090;   9 docs/s; 116429 sec
[2024-04-17 01:47:35,223 INFO] Step 49800/50000; xent: 2.65; lr: 0.0000090;   9 docs/s; 116547 sec
[2024-04-17 01:49:30,908 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.136.bert.pt, number of examples: 2001
[2024-04-17 01:49:34,767 INFO] Step 49850/50000; xent: 2.60; lr: 0.0000090;   9 docs/s; 116666 sec
[2024-04-17 01:51:31,745 INFO] Step 49900/50000; xent: 2.62; lr: 0.0000090;   9 docs/s; 116783 sec
[2024-04-17 01:53:29,305 INFO] Step 49950/50000; xent: 2.65; lr: 0.0000089;   9 docs/s; 116901 sec
[2024-04-17 01:55:27,963 INFO] Step 50000/50000; xent: 2.68; lr: 0.0000089;   9 docs/s; 117019 sec
[2024-04-17 01:55:27,972 INFO] Saving checkpoint D:\MSA\GRAD\NewsArticlesSummarizer\saved_models_extractive_setting\model_step_50000.pt
[2024-04-17 01:55:31,012 INFO] Loading train dataset from D:\MSA\GRAD\NewsArticlesSummarizer\bert_data\cnndm.train.75.bert.pt, number of examples: 1999
